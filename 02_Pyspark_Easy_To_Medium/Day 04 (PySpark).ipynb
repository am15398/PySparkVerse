{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de613a38-27c9-415b-97c3-c9d1f695fa9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48cc13bd-b8d1-4da5-a7cb-27f9ab31288e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<h2> The PADS Hacker Rank </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d9623a4-6828-4698-a0ed-607e1818fb58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n|     Name|Occupation|\n+---------+----------+\n|   Ashley| Professor|\n| Samantha|     Actor|\n|    Julia|    Doctor|\n|  Britney| Professor|\n|    Maria| Professor|\n|    Meera| Professor|\n|    Priya|    Doctor|\n| Priyanka| Professor|\n| Jennifer|     Actor|\n|    Ketty|     Actor|\n|   Belvet| Professor|\n|    Naomi| Professor|\n|     Jane|    Singer|\n|    Jenny|    Singer|\n| Kristeen|    Singer|\n|Christeen|    Singer|\n|      Eve|     Actor|\n|   Aamina|    Doctor|\n+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "data=[(\"Ashley\",\"Professor\"),(\"Samantha\",\"Actor\"),(\"Julia\",\"Doctor\"),(\"Britney\",\"Professor\"),(\"Maria\",\"Professor\"),(\"Meera\",\"Professor\"),(\"Priya\",\"Doctor\"),(\"Priyanka\",\"Professor\"),(\"Jennifer\",\"Actor\"),(\"Ketty\",\"Actor\"),(\"Belvet\",\"Professor\"),(\"Naomi\",\"Professor\"),(\"Jane\",\"Singer\"),(\"Jenny\",\"Singer\"),(\"Kristeen\",\"Singer\"),(\"Christeen\",\"Singer\"),(\"Eve\",\"Actor\"),(\"Aamina\",\"Doctor\")]\n",
    "schema =[\"Name\",\"Occupation\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82c975b6-3d8d-4fab-8aac-a49cb03d9406",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<h3> 1. PySpark code to alphabetically ordered list of all names in OCCUPATIONS, immediately followed by the first letter of each profession as a parenthetical (i.e.: enclosed in parentheses). For example: AnActorName(A), ADoctorName(D), AProfessorName(P), and ASingerName(S) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db6512c9-31a5-46c1-8c94-0368db9fcc3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|        Name|\n+------------+\n|   Aamina(D)|\n|   Ashley(P)|\n|   Belvet(P)|\n|  Britney(P)|\n|Christeen(S)|\n|      Eve(A)|\n|     Jane(S)|\n| Jennifer(A)|\n|    Jenny(S)|\n|    Julia(D)|\n|    Ketty(A)|\n| Kristeen(S)|\n|    Maria(P)|\n|    Meera(P)|\n|    Naomi(P)|\n|    Priya(D)|\n| Priyanka(P)|\n| Samantha(A)|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Name\",\n",
    "    F.concat(F.col(\"Name\"), F.lit(\"(\"), F.substring(F.col(\"Occupation\"), 1, 1),F.lit(\")\")))\\\n",
    "        .orderBy(\"Name\").select(\"Name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bc95e11-c741-4848-9064-c543641b296c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<h3> 2. PySpark code, the number of ocurrences of each occupation in OCCUPATIONS. Sort the occurrences in ascending order </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fabe1ac6-c010-492f-9f72-f8055b751384",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n|totals                            |\n+----------------------------------+\n|There are a total of 3 doctors.   |\n|There are a total of 4 actors.    |\n|There are a total of 4 singers.   |\n|There are a total of 7 professors.|\n+----------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"occupation\").agg(\n",
    "    F.count(\"occupation\").alias(\"count\")\n",
    ").withColumn(\"totals\",\n",
    "    F.concat(F.lit(\"There are a total of \"), F.col(\"count\"), F.lit(\" \"), F.lower(F.col(\"occupation\")), F.lit( \"s.\"))\n",
    ").orderBy(\"totals\").select(\"totals\").show(truncate =False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day 04 (PySpark)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
