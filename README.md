# PySparkVerse

Welcome to the PySparkVerse repository! This repository contains a collection of PySpark code snippets and examples categorized by day for easy reference and learning. Whether you're new to PySpark or looking to deepen your understanding, you'll find a variety of topics covered here.

- **LinkedIn** [AYUSH MAURYA](https://www.linkedin.com/in/ayush-maurya4/)
- **LinkedIn** [ABHISHEK KUMAR](https://www.linkedin.com/in/abhishek72/)

# PYTHON
-------------------------------------------------------------------------------------------------------------------------------------------------------------------

# "Hi, I found this Python somewhere on Google. Feel free to take credit for it."
## 01_Python_basic
- [**00 ComparisonOperators_Assert**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_00%20ComparisonOperators_Assert.ipynb)
- [**01 python_variables**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_01%20python_variables.ipynb)
- [**02 Python_datatype**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_02%20Python_datatypes.ipynb)
- [**03 Python_If_statements**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_03%20Python_If_statements.ipynb)
- [**04 Loops**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_04%20Loops.ipynb)
- [**05 Funcitons**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_05%20Funcitons.ipynb)
- [**06 Python_list**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_06%20Python_list.ipynb)
- [**07 Python_tuple**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_07%20Python_tuple.ipynb)
- [**08 Python_sets**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_08%20Python_sets.ipynb)
- [**09 Python_dictionaries**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_09%20Python_dictionaries.ipynb)
- [**10 Lamda_funcion**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_10%20Lamda_funcion.ipynb)
- [**11 Local_Global_Variables**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_11%20Local_Global_Variables.ipynb)
- [**12 exception_handling**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_12%20exception_handling.ipynb)
- [**13 Strucutres_Examples**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_13%20Strucutres_Examples.ipynb)
- [**14 Functions_Examples_1**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_14%20Functions_Examples_1.ipynb)
- [**14 Functions_Examples_3**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_14%20Functions_Examples_3.ipynb)
- [**15 Files_IO**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_15%20Files_IO.ipynb)
- [**16 Regular_expressions**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_16%20Regular_expressions.ipynb)
- [**17 Regular_Complete**](https://github.com/am15398/PySparkVerse/blob/main/Python/01%20Python%20Basic/Tutorial_17%20Regular_Complete.ipynb)

## 02_Python_DS
- [**01 List**](https://github.com/am15398/Python_practice/blob/main/02_Python_DS/01%20List.ipynb)
- [**02 Dict**](https://github.com/am15398/Python_practice/blob/main/02_Python_DS/02%20Dict.ipynb)
- [**03 Tuple**](https://github.com/am15398/Python_practice/blob/main/02_Python_DS/03%20Tuple.ipynb)
- [**04 Set**](https://github.com/am15398/Python_practice/blob/main/02_Python_DS/04%20Set.ipynb)
- [**05 Frozenset**](https://github.com/am15398/Python_practice/blob/main/02_Python_DS/05%20frozenset.ipynb)
- [**06 String**](https://github.com/am15398/Python_practice/blob/main/02_Python_DS/06%20String.ipynb)
- [**07 Collections Module**](https://github.com/am15398/Python_practice/blob/main/02_Python_DS/07%20Collections%20Module.ipynb)

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

# PYSPARK 

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

 ## Table of Contents
 ## [01_PySpark_Basic](#01_pyspark_basic-in-this-lesson-we-will-learn-how-to-perform-transformations-in-pyspark)
- [Day 1](#day-1-)
- [Day 2](#day-2-)
- [Day 3](#day-3-)
- [Day 4](#day-4-)
- [Day 5](#day-5-)
- [Day 6](#day-6-)
- [Day 7](#day-7-)
- [Day 8](#day-8-)
- [Day 9](#day-9-)
  
## [02_Pyspark_Easy_To_Medium](#02_pyspark_easy_to_medium-in-this-lesson-we-will-solve-questions-ranging-from-easy-to-medium-difficulty-level-we-have-taken-references-from-hackerrank)
- [Day_1](#day_1-)
- [Day_2](#day_2-)
- [Day_3](#day_3-)
- [Day_4](#day_4-)
- [Day_5](#day_5-)
- [Day_6](#day_6-)
- [Day_7](#day_7-)
- [Day_8](#day_8-)
- [Day_9](#day_9-)
- [Day_10](#day_10-)

## [03_pyspark_hard_problems](#03_pyspark_hard_problems-in-this-lesson-we-will-solve-questions-hard-difficulty-level-we-have-taken-references-from-ankit-bansal)
- **Heartfelt gratitude to** [**Ankit Bansal.**](https://www.linkedin.com/in/ankitbansal6/)

## 01_PySpark_Basic (In this lesson, we will learn how to perform transformations in PySpark)

## Day 1 :
[Redirect to Day 1 notebook](https://github.com/am15398/PySparkVerse/blob/main/01_PySpark_Baisc/Day%201%20(PySpark).ipynb)

- Get count in DataFrame
- Select columns in DataFrame
- Filter Rows in DataFrame
- Avg of column & Alias in DataFrame
- Group by in DataFrame
- Order by in DataFrame
- Join in DataFrame
- Using Alias in join in DataFrame
- Union All in DataFrame
- Union in DataFrame
- Distinct in DataFrame
- DropDuplicates in DataFrame
- Limit in DataFrame
- New Column in DataFrame
- Filter on multiple column in DataFrame
- Subquery in DataFrame
- Between in DataFrame
- Like in DataFrame
- Case in DataFrame
- Cast data type
- CountDistinct in DataFrame
- Substring in DataFrame
- Lit in DataFrame
- Concat column in DataFrame
- String concatenation in DataFrame

[ðŸ”¼ Back to top](#01_pyspark_basic)

## Day 2 :
[Redirect to Day 2 notebook](https://github.com/am15398/PySparkVerse/blob/main/01_PySpark_Baisc/Day%202%20(PySpark).ipynb)
- Average over partition in DataFrame
- Sum over partition in DataFrame
- Lead function with default value in DataFrame
- Lag function with default value in DataFrame
- Drop column in DataFrame
- Rename column in DataFrame
- Change column datatype in DataFrame

[ðŸ”¼ Back to top](#01_pyspark_basic)

## Day 3 :
[Redirect to Day 3 notebook](https://github.com/am15398/PySparkVerse/blob/main/01_PySpark_Baisc/Day%203%20(PySpark).ipynb)
- Creating table from DataFrame: Learn how to create a table from a DataFrame in PySpark.
- Insert data in DataFrame: Understand how to insert data into a DataFrame.
- Create table with specific column in DataFrame: Learn to create a table with specific columns from a DataFrame.
- Aggregate with alias in DataFrame: Perform aggregations with aliases in PySpark DataFrames.
- Nested subquery in DataFrame: Explore nested subqueries in PySpark DataFrames.
- Cross join in DataFrame: Learn how to perform cross joins between DataFrames.
- Group by having count greater than in DataFrame: Group by clauses with conditions in PySpark DataFrames.
- Alias for table join (default is inner) in DataFrame: Specify aliases for table joins in PySpark DataFrames.
- Select from multiple tables in DataFrame: Select data from multiple tables using PySpark DataFrames.

[ðŸ”¼ Back to top](#01_pyspark_basic)

## Day 4 :
[Redirect to Day 4 notebook](https://github.com/am15398/PySparkVerse/blob/main/01_PySpark_Baisc/Day%204%20(PySpark).ipynb)
- Extract date part in DataFrame: Extract specific parts of a date from a DataFrame in PySpark.
- Inequality filtering in DataFrame: Filter DataFrame based on inequality conditions.
- In list in DataFrame: Filter DataFrame where a column value is in a list of values.
- Not in list in DataFrame: Filter DataFrame where a column value is not in a list of values.
- Null values in DataFrame: Handle null values in PySpark DataFrames.
- Not null values in DataFrame: Filter DataFrame to retrieve non-null values.
- Upper case in DataFrame: Convert column values to uppercase in PySpark DataFrames.
- Lower case in DataFrame: Convert column values to lowercase in PySpark DataFrames.
- Length in DataFrame: Calculate the length of strings in DataFrame columns.
- Trim case in DataFrame: Trim whitespace from the beginning and end of strings in DataFrame columns.
- Ltrim case in DataFrame: Trim whitespace from the beginning of strings in DataFrame columns.
- Rtrim case in DataFrame: Trim whitespace from the end of strings in DataFrame columns.
- String replace in DataFrame: Replace occurrences of a substring in DataFrame columns.
- Coalesce in DataFrame: Select the first non-null value from a list of columns in PySpark DataFrames.
- Date diff in DataFrame: Calculate the difference between two dates in PySpark DataFrames.
- Add months to date in DataFrame: Add a specified number of months to a date in PySpark DataFrames.

[ðŸ”¼ Back to top](#01_pyspark_basic)

## Day 5 :
[Redirect to Day 5 notebook](https://github.com/am15398/PySparkVerse/blob/main/01_PySpark_Baisc/Day%205%20(PySpark).ipynb)
- First value in group in DataFrame: Get the first value in each group of a DataFrame.
- Last value in group in DataFrame: Get the last value in each group of a DataFrame.
- Row number over partition in DataFrame: Assign a unique row number to each row within a partition of a DataFrame.
- Rank number over partition in DataFrame: Assign a rank to each row within a partition of a DataFrame.
- Dense rank number over partition in DataFrame: Assign a dense rank to each row within a partition of a DataFrame.
- Min value in group in DataFrame: Find the minimum value in each group of a DataFrame.
- Min value in table in DataFrame: Find the minimum value in a DataFrame column.
- Max value in group in DataFrame: Find the maximum value in each group of a DataFrame.
- Max value in table in DataFrame: Find the maximum value in a DataFrame column.

[ðŸ”¼ Back to top](#01_pyspark_basic)

## Day 6 :
[Redirect to Day 6 notebook](https://github.com/am15398/PySparkVerse/blob/main/01_PySpark_Baisc/Day%206%20(PySpark).ipynb)
- Left join in DataFrame: Perform a left join between two DataFrames in PySpark.
- Right join in DataFrame: Perform a right join between two DataFrames in PySpark.
- Outer join in DataFrame: Perform an outer join between two DataFrames in PySpark.
- Group by having in DataFrame: Filter groups using the HAVING clause in PySpark DataFrames.
- Round decimal value in DataFrame: Round decimal values in DataFrame columns to a specified number of decimal places.
- Today date in DataFrame: Retrieve the current date in PySpark DataFrames.
- Date addition in DataFrame: Add a specified number of days to a date in PySpark DataFrames.
- Date subtract in DataFrame: Subtract a specified number of days from a date in PySpark DataFrames.
- Year from date in DataFrame: Extract the year component from a date in PySpark DataFrames.
- Month from date in DataFrame: Extract the month component from a date in PySpark DataFrames.
- Day from date in DataFrame: Extract the day component from a date in PySpark DataFrames.
- Sorting in DataFrame: Sort DataFrame by one or more columns in ascending or descending order.

[ðŸ”¼ Back to top](#01_pyspark_basic)
  
## Day 7 :
[Redirect to Day 7 notebook](https://github.com/am15398/PySparkVerse/blob/main/01_PySpark_Baisc/Day%207%20(PySpark).ipynb)
- dbutils.help(): Learn how to use dbutils.help() to get help on available functions in Databricks.
- dbutils.fs.help(): Explore dbutils.fs.help() to get help on file system operations in Databricks.
- Read files from folder: Read files from a folder into a DataFrame in PySpark.
- Today date: Retrieve the current date in PySpark DataFrames.
- Creating spark session: Learn how to create a SparkSession in PySpark.
- Read CSV file with header and schema: Read a CSV file with header and schema into a DataFrame.
- Read CSV file with skip 5 rows: Read a CSV file skipping the first 5 rows into a DataFrame.
- Dropping rows with missing value: Drop rows with missing values from a DataFrame.
- Fill null value: Fill null values in a DataFrame with specified values.
- Writing to parquet: Write DataFrame to Parquet file format.
- Broadcast join: Perform a broadcast join in PySpark.
- caching data : learn how to cache dataframe data 
- Get number of partitions: Get the number of partitions in a DataFrame.
- Increase the partition: Increase the number of partitions in a DataFrame.
- Decrease the partition: Decrease the number of partitions in a DataFrame.
- repartitionByRange: Repartition DataFrame by range into a specified number of partitions.
- Show the data: Display the contents of a DataFrame.
- Explain plan: Display the execution plan for a DataFrame.
- Read CSV file with permissive: Read a CSV file with permissive mode into a DataFrame.
- Read CSV file with DROPMALFORMED: Read a CSV file with DROPMALFORMED mode into a DataFrame.
- Read CSV file with FAILFAST: Read a CSV file with FAILFAST mode into a DataFrame.
- Read CSV file with permissive capture bad record: Read a CSV file with permissive mode and capture bad records into a DataFrame.
- Explode function (Array): Explode an array column into multiple rows in a DataFrame.
- Struct Field: Define struct fields in PySpark DataFrames.
- HASH (MD5): Calculate MD5 hash values for DataFrame columns.
- PySpark UDF: Define and use User Defined Functions (UDFs) in PySpark.
- Load data to delta table: Load data into a Delta table.
- Describe the detail of the table: Describe the schema of a table in PySpark.
- Get column details: Get details of columns in a PySpark DataFrame.
- Insert new row: Insert a new row into a DataFrame.
- Get the history of table: Get the history of changes made to a Delta table.
- Time travel feature: Use Delta's time travel feature to query historical versions of a table.
- Cache: Cache DataFrame for better performance.
- Analyze: Analyze DataFrame for better performance.
- Optimize the table: Optimize a Delta table for better performance.
- Optimize / Zorder: Z-order DataFrame for better query performance.
- Vacuum: Vacuum a Delta table to reclaim space by removing old versions of files.

[ðŸ”¼ Back to top](#01_pyspark_basic)

## Day 8 :
[Redirect to Day 8 notebook](https://github.com/am15398/PySparkVerse/blob/main/01_PySpark_Baisc/Day%208%20(PySpark).ipynb)
- Read multiple files with filename in a new column.
- Union DataFrames with different schemas.
- merge schema true example in Pyspark.
- Struct col.

[ðŸ”¼ Back to top](#01_pyspark_basic)

## Day 9 :
[Redirect to Day 9 notebook](https://github.com/am15398/PySparkVerse/blob/main/01_PySpark_Baisc/Day%209%20(Pyspark).ipynb)
- AVG
- MAX
- MIN
- SUM
- COUNT
- ROW_NUMBER
- RANK
- DENSE_RANK
- PERCENT_RANK
- NTILE
- LAG
- LEAD
- FIRST_VALUE
- LAST_VALUE
- NTH_VALUE

[ðŸ”¼ Back to top](#01_pyspark_basic)

## 02_Pyspark_Easy_To_Medium (In this lesson, we will solve questions ranging from easy to medium difficulty level. We have taken references from HackerRank)

## Day_1 :
[Redirect to Day 1 notebook](https://github.com/am15398/PySparkVerse/blob/main/02_Pyspark_Easy_To_Medium/Day%2001%20(PySpark).ipynb)
- Revising the Select Query I Hacker Rank
- Revising the Select Query II Hacker Rank
- Select All Hacker Rank
- Select By ID Hacker Rank
- Japanese Cities' Attributes Hacker Rank
- Japanese Cities' Names Hacker Rank

[ðŸ”¼ Back to top](#02_Pyspark_Easy_To_Medium)

## Day_2 :
[Redirect to Day 2 notebook](https://github.com/am15398/PySparkVerse/blob/main/02_Pyspark_Easy_To_Medium/Day%2002%20(PySpark).ipynb)
- Weather Observation Station 1 Hacker Rank
- Weather Observation Station 3 Hacker Rank
- Weather Observation Station 4 Hacker Rank
- Weather Observation Station 6 & 7 Hacker Rank
- Weather Observation Station 8 Hacker Rank
- Weather Observation Station 9 Hacker Rank
- Weather Observation Station 10 Hacker Rank
- Weather Observation Station 11 Hacker Rank
- Weather Observation Station 12 Hacker Rank

[ðŸ”¼ Back to top](#02_Pyspark_Easy_To_Medium)

## Day_3 :
[Redirect to Day 3 notebook](https://github.com/am15398/PySparkVerse/blob/main/02_Pyspark_Easy_To_Medium/Day%2003%20(Pyspark).ipynb)
- Higher Than 75 Marks Hacker Rank
- Employee Names Hacker Rank
- Employee Salaries Hacker Rank
- Type of Triangle Hacker Rank

[ðŸ”¼ Back to top](#02_Pyspark_Easy_To_Medium)

## Day_4 :
[Redirect to Day 4 notebook](https://github.com/am15398/PySparkVerse/blob/main/02_Pyspark_Easy_To_Medium/Day%2004%20(PySpark).ipynb)
- The PADS Hacker Rank ( Q1 & Q2 )

[ðŸ”¼ Back to top](#02_Pyspark_Easy_To_Medium)

## Day_5 :
[Redirect to Day 5 notebook](https://github.com/am15398/PySparkVerse/blob/main/02_Pyspark_Easy_To_Medium/Day%2005%20(PySpark).ipynb)
- Revising Aggregations - The Count Function Hacker Rank
- Revising Aggregations - The Sum Function Hacker Rank
- Revising Aggregations - Averages Hacker Rank
- Average Population Hacker Rank
- Japan Population Hacker Rank
- Population Density Difference Hacker Rank
- The Blunder Hacker Rank
- Top Earners Hacker Rank

[ðŸ”¼ Back to top](#02_Pyspark_Easy_To_Medium)

## Day_6 :
[Redirect to Day 6 notebook](https://github.com/am15398/PySparkVerse/blob/main/02_Pyspark_Easy_To_Medium/Day%2006%20(PySpark).ipynb)
- Weather Observation Station 2 Hacker Rank
- Weather Observation Station 13 Hacker Rank
- Weather Observation Station 14 Hacker Rank
- Weather Observation Station 15 Hacker Rank
- Weather Observation Station 16 Hacker Rank
- Weather Observation Station 17 Hacker Rank
- Weather Observation Station 18 Hacker Rank
- Weather Observation Station 19 Hacker Rank

[ðŸ”¼ Back to top](#02_Pyspark_Easy_To_Medium)

## Day_7 :
[Redirect to Day 7 notebook](https://github.com/am15398/PySparkVerse/blob/main/02_Pyspark_Easy_To_Medium/Day%2007%20(PySpark).ipynb)
- Population Census Hacker Rank
- African Cities Hacker Rank
- Average Population of Each Continent

[ðŸ”¼ Back to top](#02_Pyspark_Easy_To_Medium)

## Day_8 :
[Redirect to Day 8 notebook](https://github.com/am15398/PySparkVerse/blob/main/02_Pyspark_Easy_To_Medium/Day%2008%20(PySpark).ipynb)
- Binary Tree Nodes Hacker Rank
- New Companies Hacker Rank

[ðŸ”¼ Back to top](#02_Pyspark_Easy_To_Medium)

## Day_9 :
[Redirect to Day 9 notebook](https://github.com/am15398/PySparkVerse/blob/main/02_Pyspark_Easy_To_Medium/Day%2009%20(PySpark).ipynb)
- The Report Hacker Rank
- Top Competitors Hacker Rank

[ðŸ”¼ Back to top](#02_Pyspark_Easy_To_Medium)

## Day_10 :
[Redirect to Day 10 notebook](https://github.com/am15398/PySparkVerse/blob/main/02_Pyspark_Easy_To_Medium/Day%2010%20(PySpark).ipynb)
- Ollivander's Inventory Hacker Rank
- Placements Hacker Rank

[ðŸ”¼ Back to top](#02_Pyspark_Easy_To_Medium)

## 03_pyspark_hard_problems (In this lesson, we will solve questions hard difficulty level. We have taken references from Ankit Bansal)

## Day 01 Complex SQL :
- [Ankit Bansal Complex SQL Query 1 | Derive Points table for ICC tournament](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2001%20Complex%20SQL.ipynb)
## Day 02 Complex SQL :
- [Ankit Bansal Complex SQL 2 | find new and repeat customers](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2002%20Complex%20SQL.ipynb)
## Day 03 Complex SQL :
- [Ankit Bansal Complex SQL 3 | Scenario based Interviews Question for Product companies](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2003%20Complex%20SQL.ipynb)
## Day 04 Complex SQL :
-  [Complex SQL 6 | Scenario based on join, group by and having clauses | SQL Interview Question](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2004%20Complex%20SQL.ipynb)
## Day 05 Complex SQL :
-  [Leetcode Hard Problem | Complex SQL 7 | Trips and Users](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2005%20Complex%20SQL.ipynb)
## Day 06 Complex SQL :
-  [Leetcode Hard problem 2| Tournament Winners | Complex SQL 8](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2006%20Complex%20SQL.ipynb)
## Day 07 Complex SQL :
-  [Leetcode Hard Problem 3 | Market Analysis 2 | Complex SQL 9](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2007%20Complex%20SQL.ipynb)
## Day 08 Complex SQL :
-  [An Awesome Tricky SQL Logic | Complex SQL 10](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2008%20Complex%20SQL.ipynb)
## Day 09 Complex SQL :
-   [Leetcode Hard Problem 4 | User Purchase Platform | Complex SQL 11](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2009%20Complex%20SQL.ipynb)
## Day 10 Complex SQL :
-  [Data Science SQL Interview Question | Recommendation System | Complex SQL 13](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2010%20Complex%20SQL.ipynb)
## Day 11 Complex SQL :
-  [Amazon Prime Subscription Rate SQL Logic | Amazon Music | Complex SQL 14](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2011%20Complex%20SQL.ipynb)
## Day 12 Complex SQL :
-  [Customer Retention and Churn Analysis (Part 1/2) | SQL Interview Question Product](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2012%20Complex%20SQL.ipynb)
## Day 13 Complex SQL :
-  [Customer Retention and Churn Analysis (Part 2/2) | SQL Interview Question Product](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2013%20Complex%20SQL.ipynb)
## Day 14 Complex SQL :
-  [Leetcode Hard SQL Problem - 6 | Second Most Recent Activity | SQL Window Analytical Functions](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2014%20Complex%20SQL.ipynb)
## Day 15 Complex SQL :
-  [Scenario Based SQL Question | Solving Using SCD Type 2 Concept | SQL Interview](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2015%20Complex%20SQL.ipynb)
## Day 16 Complex SQL :
-  [Data Analyst Spotify Case Study | SQL Interview Questions](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2016%20Complex%20SQL.ipynb)
## Day 17 Complex SQL :
-  [How to Write Advance SQL Queries | Consecutive Empty Seats | SQL Interview Questions](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2017%20Complex%20SQL.ipynb)
## Day 18 Complex SQL :
-  [SQL Interview Question and Answers | Find Missing Quarter](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2018%20Complex%20SQL.ipynb)
## Day 19 Complex SQL :
-  [Deadly Combination of Group By and Having Clause in SQL | SQL Interview Questions and Answers](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2019%20Complex%20SQL.ipynb)
## Day 20 Complex SQL :
-  [Beauty of SQL RANK Function | SQL Interview Question and Answers | Covid Cases](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2020%20Complex%20SQL.ipynb)
## Day 21 Complex SQL :
-  [Google SQL Interview Question for Data Analyst Position](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2021%20Complex%20SQL.ipynb)
## Day 22 Complex SQL :
-  [Meesho HackerRank SQL Interview Question and Answer | Customer Budget](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2022%20Complex%20SQL.ipynb)
## Day 23 Complex SQL :
-  [Horizontal Sorting in SQL | Amazon Interview Question for BIE position](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2023%20Complex%20SQL.ipynb)
## Day 24 Complex SQL :
- [Solving 4 Tricky SQL Problems](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2024%20Complex%20SQL.ipynb)
## Day 25 Complex SQL :
- [Brilliant SQL Interview Question | Solve it without using CTE, Sub Query, Window functions](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2025%20Complex%20SQL.ipynb)
## Day 26 Complex SQL :
- [Solving A Hard SQL Problem | SQL ON OFF Problem | Magic of SQL](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2026%20Complex%20SQL.ipynb)
## Day 27 Complex SQL :
- [LeetCode Hard SQL problem | Students Reports By Geography | Pivot Ka Baap](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2027%20Complex%20SQL.ipynb)
## Day 28 Complex SQL :
- [I Asked This SQL Interview Question in an Amazon Interview | Most Asked SQL Problem with a Twist](https://github.com/am15398/PySparkVerse/blob/main/03_pyspark_hard_problems/Day%2028%20Complex%20SQL.ipynb)

[ðŸ”¼ Back to top](#03_pyspark_hard_problems)

## Usage
Each day's section contains a list of tasks along with the corresponding code snippets. You can simply navigate to the desired day to find the code examples you're interested in.

## Contribution
Contributions are welcome! If you have additional PySpark code snippets or improvements to existing ones, feel free to open a pull request.

## License
This repository is licensed under the [MIT License](LICENSE).
