{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12ad15d6-8a45-46c8-af5b-6edc383a9d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DataBricks Cluster \n",
    "- In Databricks, clusters are the compute engines that run your code. Databricks supports several types of clusters, each optimized for specific use cases.\n",
    "\n",
    "## üß± Databricks Cluster Types and Instance Pool (Table Format)\n",
    "\n",
    "| **Cluster Type**                    | **Details and Use Cases**                                                                                                                                                      | **Detail**                                                                                                                                                                        |\n",
    "|------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Interactive Cluster (All-Purpose)** | - **Best For:** Notebooks, development, exploration, ML experiments  <br> - **Lifespan:** Manual start/stop  <br> - **UI Access:** Workspace ‚Üí Compute                                    | Provisioned compute used to analyze data in notebooks. You can create, terminate, and restart this compute using the UI, CLI, or REST API. mainly for development |\n",
    "| **Job Cluster**                    | - **Usage:** Scheduled ETL or Batch jobs and workflows  <br> - **Lifespan:** Auto-created at job start, terminated after job completion  <br> - **Provisioned via:** Jobs API, UI         | Automatically created by Databricks when a job starts. Automatically terminates when the job finishes. Mainly used by prod jobs  |\n",
    "| **SQL Warehouse** (SQL Endpoint)   | - **Usage:** BI dashboards, SQL editor queries  <br> - **Photon-powered:** Yes (by default)  <br> - Auto-scaling and on-demand start  | Used to run SQL queries in dashboards or interactive notebooks.  |\n",
    "| **Shared Cluster**                 | - Manually configured to allow multiple users or jobs to share a single cluster                                                                                                 | Option available under All-Purpose cluster to allow multiple users to share the same cluster.                                                                                      |\n",
    "| **Single Node Cluster**           | - **Usage:** Non-distributed jobs, model training, file processing  <br> - **Configuration:** Enable \"Single node\" in cluster settings      | Created under Unrestricted policy. Suitable for lightweight and simple jobs.       |\n",
    "| **Delta Live Tables Cluster**      | - **Usage:** DLT pipelines (ETL workflows / managed pipelines)  <br> - **Managed Automatically:** Databricks provisions and scales cluster                                              | Managed via **Workflows ‚Üí Delta Live Tables**. Cluster mode options include: <br> - Default (Managed) <br> - Photon <br> - Single-node.         |\n",
    "| **Photon Cluster**                 | - **Usage:** Accelerated SQL performance  <br> - **Compatibility:** Spark SQL, Databricks SQL  <br> - **Hardware:** Optimized for x86 vectorized execution                                 | High-performance engine (Photon) used for speeding up SQL query execution. Enabled by default in SQL Warehouses and configurable in clusters.                                     |\n",
    "| **Serverless Cluster**            | - **Usage:** Auto-scaling and zero management  <br> - **Benefit:** Pay only for execution time  <br> - **Note:** Availability depends on region/plan     | Databricks provides one low-config serverless cluster per workspace for quick Python or SQL tasks. No manual setup required.    |\n",
    "| **Instance Pool**                 | - **Purpose:** Speed up cluster startup, improve efficiency  <br> - **Use with:** All-purpose and job clusters  <br> - **Benefit:** Lower cost, faster jobs, better control                | Pre-creates and manages VMs that can be reused by multiple clusters to reduce startup time and optimize cost.                                                                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e62e14a1-d434-46b4-8d89-b91f68009a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ‚öôÔ∏è Init Scripts in Databricks\n",
    "- An init script (**initialization script**) is a **shell script** that runs during startup of each cluster node before the Apache Spark driver or executor JVM starts.\n",
    "\n",
    "## üîß Common Uses:\n",
    "- Install custom libraries or dependencies\n",
    "- Mount external volumes or secrets\n",
    "- Set environment variables\n",
    "- Modify Spark or JVM configurations\n",
    "- Download system tools or install Python packages\n",
    "\n",
    "\n",
    "## üîß types of init scripts \n",
    "- **Cluster-scoped:** run on every cluster configured with the script. This is the recommended way to run an init script.\n",
    "- **Global:** run on all clusters in the workspace configured with dedicated access mode or no-isolation shared access mode. These init scripts can cause unexpected issues, such as library conflicts. Only workspace admin users can create global init scripts.\n",
    "\n",
    "\n",
    "## üõ†Ô∏è How to Create and Use an Init Script\n",
    "\n",
    "### Example: Install Python Package\n",
    "\n",
    "```bash\n",
    "\n",
    "#!/bin/bash\n",
    "/databricks/python/bin/pip install pandas-profiling\n",
    "```\n",
    "\n",
    "### Upload to DBFS or volumne or external location \n",
    "\n",
    "```python\n",
    "\n",
    "dbutils.fs.put(\"dbfs:/databricks/init/install_profiler.sh\", \"<script content>\", True)\n",
    "```\n",
    "\n",
    "### Attach to Cluster\n",
    "\n",
    "1. Go to **Cluster > Advanced Options > Init Scripts**\n",
    "2. Add path: `dbfs:/databricks/init/install_profiler.sh`\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Benefits of Init Scripts\n",
    "\n",
    "| Benefit                  | Description                                 |\n",
    "| ------------------------ | ------------------------------------------- |\n",
    "| üîÑ Customization         | Configure node behavior before Spark starts |\n",
    "| ü§ù Consistency           | Reproduce environment across all nodes      |\n",
    "| ‚öñÔ∏è Dependency Management | Install packages, Python libs, or drivers   |\n",
    "| üîê Secrets Integration   | Use Vault or Key Vault for secure access    |\n",
    "| üöÄ Enable External Tools | Monitoring agents, logging, etc.            |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö† Limitations of Init Scripts\n",
    "\n",
    "| Limitation             | Description                              |\n",
    "| ---------------------- | ---------------------------------------- |\n",
    "| ‚è≥ Startup Delay        | Long scripts slow down cluster launch    |\n",
    "| ‚ùå Limited Debugging    | Hard to trace failures without logging   |\n",
    "| üß° Scaling Issues      | Hard to manage many scripts manually     |\n",
    "| üìÑ Bash Only           | No direct Python support                 |\n",
    "| ‚ùé Immutable at Runtime | Cannot re-run without restarting cluster |\n",
    "\n",
    "---\n",
    "\n",
    "## üìí Summary\n",
    "\n",
    "| Feature   | Description                                       |\n",
    "| --------- | ------------------------------------------------- |\n",
    "| What      | Bash script run on each node before cluster start |\n",
    "| Use Cases | Install tools, mount volumes, configure system    |\n",
    "| Types     | Cluster-scoped, global, DBFS, workspace, cloud    |\n",
    "| Benefits  | Reusable, automated, secure setup                 |\n",
    "| Drawbacks | Slower startup, debugging, shell-only             |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b6e128a-990f-430c-8696-5217d0026194",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üñäÔ∏è Python Logger in Databricks\n",
    "\n",
    "Python's built-in `logging` module is a powerful tool for structured and level-based logging in **Databricks notebooks and production pipelines**. It is far more flexible and maintainable than using simple `print()` statements.\n",
    "\n",
    "The `logging` module in Python allows you to:\n",
    "\n",
    "* Record events and messages during execution\n",
    "* Classify messages by severity (INFO, ERROR, etc.)\n",
    "* Send logs to different outputs (console, files, etc.)\n",
    "* Control log formatting, level, and filtering\n",
    "\n",
    "---\n",
    "\n",
    "## üîí Logging Levels\n",
    "\n",
    "| Level      | Use Case                         |\n",
    "| ---------- | -------------------------------- |\n",
    "| `DEBUG`    | Verbose output for developers    |\n",
    "| `INFO`     | General runtime events           |\n",
    "| `WARNING`  | Recoverable issues               |\n",
    "| `ERROR`    | Runtime errors, can continue     |\n",
    "| `CRITICAL` | Fatal errors, aborting execution |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Benefits of Using Logger in Databricks\n",
    "\n",
    "| Benefit              | Description                                           |\n",
    "| -------------------- | ----------------------------------------------------- |\n",
    "| üìä Structured Output | Includes timestamp, severity, etc.                    |\n",
    "| üîÑ Reusability       | Can be reused across jobs and notebooks               |\n",
    "| üö® Better Debugging  | Helps trace errors and warnings in production         |\n",
    "| üìÇ File Logging      | Logs can be saved for audits or monitoring            |\n",
    "| üöÄ Scalable          | Can log to multiple destinations (DBFS, stdout, etc.) |\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Common Use Cases\n",
    "\n",
    "| Scenario          | Description                                       |\n",
    "| ----------------- | ------------------------------------------------- |\n",
    "| ETL Pipelines     | Track each stage of ingestion/transformation      |\n",
    "| Data Validation   | Log schema mismatches or null checks              |\n",
    "| ML Model Training | Log metrics, hyperparameters, convergence         |\n",
    "| Alerts            | Warn on threshold breaches (e.g., null rate > 5%) |\n",
    "| Auditing          | Log user actions or sensitive data access         |\n",
    "\n",
    "---\n",
    "\n",
    "## üí™ Best Practices\n",
    "\n",
    "* Use `logger` over `print()` in production\n",
    "* Set log level according to environment (e.g., DEBUG for dev, INFO for prod)\n",
    "* Combine with `try-except` to log errors\n",
    "* Use file rotation or cleanup jobs to manage log files\n",
    "* Keep logging setup in a shared module for consistency\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_cluster_utilties_framework",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
