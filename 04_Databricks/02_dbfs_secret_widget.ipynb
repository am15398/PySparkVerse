{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22561221-c29c-4e59-a9d7-b178974d3e91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# *** DBFS ***\n",
    "\n",
    "- ### DBFS commands used to interact with the Databricks File System (DBFS) in Azure Databricks.\n",
    "\n",
    "- DBFS commands e.g.\n",
    "---\n",
    "| **Command**                                      | **Description**                                            | **Example**                                                        |\n",
    "| ------------------------------------------------ | ---------------------------------------------------------- | ------------------------------------------------------------------ |\n",
    "| `dbutils.fs.ls(path)`                            | **Lists files and directories at the given path.**    | `dbutils.fs.ls(\"/mnt/raw-data/\")`                                  |\n",
    "| `dbutils.fs.cp(src, dst)`                        | **Copies file from `src` to `dst`.**                           | `dbutils.fs.cp(\"/mnt/src/file.txt\", \"/mnt/dest/file.txt\")`         |\n",
    "| `dbutils.fs.cp(src, dst, recurse=True)`          | **Recursively copies a folder.** ( You copy the folder itself, all of its files, and all of its subfolders and their contents, no matter how deeply nested )                               | `dbutils.fs.cp(\"/mnt/src/\", \"/mnt/dest/\", recurse=True)`           |\n",
    "| `dbutils.fs.mv(src, dst)`                        | **Moves/renames file or directory.**                           | `dbutils.fs.mv(\"/mnt/file1.txt\", \"/mnt/file2.txt\")`                |\n",
    "| `dbutils.fs.rm(path)`                            | **Deletes a file.**                                            | `dbutils.fs.rm(\"/mnt/file.txt\")`                                   |\n",
    "| `dbutils.fs.rm(path, recurse=True)`              | **Deletes a directory and its contents recursively.** ( Ensures that if the path is a directory, it will be deleted along with all its subdirectories and files )          | `dbutils.fs.rm(\"/mnt/folder/\", recurse=True)`                      |\n",
    "| `dbutils.fs.mkdirs(path)`                        | **Creates the directory structure specified.**                 | `dbutils.fs.mkdirs(\"/mnt/new-folder/\")`                            |\n",
    "| `dbutils.fs.put(path, contents)`                 | **Creates a new file at path and writes text contents to it.** | `dbutils.fs.put(\"/mnt/sample.txt\", \"Hello World!\")`                |\n",
    "| `dbutils.fs.put(path, contents, overwrite=True)` | **Overwrites the file if it exists.**                          | `dbutils.fs.put(\"/mnt/sample.txt\", \"New content\", overwrite=True)` |\n",
    "| `dbutils.fs.head(path)`                          | **Reads the first few bytes (default 65536) of the file.**     | `dbutils.fs.head(\"/mnt/sample.txt\")`                               |\n",
    "| `dbutils.fs.mounts()`                            | **Lists all mounted storage containers.**                      | `dbutils.fs.mounts()`                                              |\n",
    "| `dbutils.fs.unmount(path)`                       | **Unmounts the given mount point.**                            | `dbutils.fs.unmount(\"/mnt/raw-data\")`                              |\n",
    "|`dbutils.secrets.get()`  |  **Get the secreate details**  | `dbutils.secrets.get(scope=\"<SCOPE_NAME>\", key=\"<KEY_NAME>\")`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2a0380b-5d54-41d3-9fad-3c67cc01403f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DBFS Secreat and mount point \n",
    "\n",
    "### How to create a Databricks backed secret scope:\n",
    "- https://www.youtube.com/watch?v=vsJvriTpMYU&list=PLzU8IF5r8skukp9lOHgChlOESJLjGNBww&index=5\n",
    "\n",
    "### How to access Databricks secret scopes\n",
    "- https://www.youtube.com/watch?v=PtJCLWbP2EU&list=PLzU8IF5r8skukp9lOHgChlOESJLjGNBww&index=14\n",
    "\n",
    "### Bash command \n",
    "databricks secrets create-scope **Secreat_name**\n",
    "\n",
    "databricks secrets put-secret **Secreat_name** username\n",
    "\n",
    "databricks secrets put-secret **Secreat_name** password\n",
    "\n",
    "### Python command \n",
    "```python\n",
    "\n",
    "# Read the storage account key from Databricks secrets\n",
    "storage_account_name = \"mystorageaccount\"\n",
    "container_name = \"mycontainer\"\n",
    "mount_point = \"/mnt/mydata\"\n",
    "\n",
    "storage_key = dbutils.secrets.get(scope=\"adls-creds\", key=\"adls-key\")\n",
    "\n",
    "# Build the config dictionary\n",
    "configs = {\n",
    "  f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\": storage_key\n",
    "}\n",
    "\n",
    "# Source path\n",
    "source_uri = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\"\n",
    "\n",
    "# Mount if not already mounted\n",
    "if not any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "    dbutils.fs.mount(\n",
    "        source = source_uri,\n",
    "        mount_point = mount_point,\n",
    "        extra_configs = configs\n",
    "    )\n",
    "    print(f\"Mounted {source_uri} to {mount_point}\")\n",
    "else:\n",
    "    print(f\"{mount_point} is already mounted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15cea992-44a1-4bdd-a5b9-39299d3f7ecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 📘 Databricks Widgets \n",
    "\n",
    "Widgets in **Databricks** allow you to **parameterize notebooks**. They create interactive input controls that you can use to dynamically set values when running notebooks.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Types of Widgets\n",
    "\n",
    "## 1. Text Widget\n",
    "- Used for string inputs.\n",
    "```python\n",
    "\n",
    "dbutils.widgets.text(\"param1\", \"default_value\", \"Parameter 1\")\n",
    "\n",
    "```\n",
    "\n",
    "## 2. Dropdown Widget\n",
    "- User selects from a fixed list of values.\n",
    "```python\n",
    "\n",
    "dbutils.widgets.dropdown(\"param2\", \"A\", [\"A\", \"B\", \"C\"], \"Parameter 2\")\n",
    "\n",
    "```\n",
    "\n",
    "## 3. Combobox Widget\n",
    "- Like dropdown, but allows users to type custom values too.\n",
    "```python\n",
    "\n",
    "dbutils.widgets.combobox(\"param3\", \"X\", [\"X\", \"Y\", \"Z\"], \"Parameter 3\")\n",
    "\n",
    "```\n",
    "\n",
    "## 4. Multiselect Widget\n",
    "```python\n",
    "\n",
    "dbutils.widgets.multiselect(\"param4\", \"X\", [\"X\", \"Y\", \"Z\"], \"Parameter 4\")\n",
    "\n",
    "```\n",
    "\n",
    "## 📥 Accessing Widget Values\n",
    "- User can choose multiple values.\n",
    "```python\n",
    "\n",
    "param_value = dbutils.widgets.get(\"param1\")\n",
    "\n",
    "```\n",
    "\n",
    "## 🧹 Removing Widgets\n",
    "```python\n",
    "\n",
    "dbutils.widgets.removeAll()\n",
    "\n",
    "```\n",
    "\n",
    "## Passing widget in notebook \n",
    "\n",
    "### CHILD NOTEBOOK\n",
    "```python\n",
    "\n",
    "# Declare widgets (same names used in parent arguments)\n",
    "dbutils.widgets.text(\"state\", \"\", \"State\")\n",
    "dbutils.widgets.text(\"file_format\", \"\", \"File Format\")\n",
    "\n",
    "# Get widget values\n",
    "state = dbutils.widgets.get(\"state\")\n",
    "file_format = dbutils.widgets.get(\"file_format\")\n",
    "\n",
    "# Do something with them\n",
    "print(f\"✅ Received in child -> State: {state}, File Format: {file_format}\")\n",
    "\n",
    "# Return value back to parent (optional)\n",
    "dbutils.notebook.exit(f\"Received state={state}, file_format={file_format}\")\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### PARENT NOTEBOOK\n",
    "```python\n",
    "\n",
    "# Example metadata table\n",
    "metadata_df = spark.read.table(\"your_catalog.your_schema.metadata_table\")\n",
    "\n",
    "# Get parameters for a specific pipeline\n",
    "pipeline_name = \"pipeline_1\"\n",
    "row = metadata_df.filter(f\"pipeline_name = '{pipeline_name}'\").limit(1).collect()[0]\n",
    "\n",
    "# Extract values\n",
    "state = row[\"state\"]\n",
    "file_format = row[\"file_format\"]\n",
    "\n",
    "# Call child notebook with values\n",
    "result = dbutils.notebook.run(\n",
    "    \"/Workspace/ChildNotebook\", \n",
    "    timeout_seconds=60, \n",
    "    arguments={\"state\": state, \"file_format\": file_format}\n",
    ")\n",
    "\n",
    "print(f\"✅ Parent got response: {result}\")\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ca63e76-4c11-4ded-a7d8-01110af7f470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"text\", \"default_value\", \"Text Widget Example\")\n",
    "dbutils.widgets.dropdown(\"DropDown\", \"A\", [\"A\", \"B\", \"C\"], \"DropDown Widget Example\")\n",
    "dbutils.widgets.combobox(\"ComboBox\", \"X\", [\"X\", \"Y\", \"Z\"], \"ComboBox  Widget Example\")\n",
    "dbutils.widgets.multiselect(\"MultiSelect\", \"X\", [\"X\", \"Y\", \"Z\"], \"MultiSelect  Widget Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72828966-6c45-4f5e-a127-488128ff0e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text widget value : default_value\nDropDown widget value : A\nCombobox widget value : X\nmultiselect widget value : X\n"
     ]
    }
   ],
   "source": [
    "# Widget name is case senstive \n",
    "print(\"Text widget value :\",dbutils.widgets.get(\"text\"))\n",
    "print(\"DropDown widget value :\",dbutils.widgets.get(\"DropDown\"))\n",
    "print(\"Combobox widget value :\",dbutils.widgets.get(\"ComboBox\"))\n",
    "print(\"multiselect widget value :\",dbutils.widgets.get(\"MultiSelect\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f817fbb0-0b9f-4814-afe4-3cb7398bf489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|         col1|\n+-------------+\n|default_value|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Get selected table name\n",
    "selected_table = dbutils.widgets.get(\"text\")\n",
    "\n",
    "# Use it in SQL\n",
    "spark.sql(f\"SELECT  '{selected_table}' as col1\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cae4cacc-9971-46fa-8b79-5dfcef8eeb14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {
        "state": {
         "defaultValue": "CA",
         "label": null,
         "name": "state",
         "options": {
          "autoCreated": null,
          "choices": [
           "CA",
           "IL",
           "MI",
           "NY",
           "OR",
           "VA"
          ],
          "widgetType": "dropdown"
         },
         "widgetType": "dropdown"
        }
       },
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE WIDGET DROPDOWN state DEFAULT \"CA\" CHOICES SELECT * FROM (VALUES (\"CA\"), (\"IL\"), (\"MI\"), (\"NY\"), (\"OR\"), (\"VA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79fd21a4-ea06-4033-a7a1-2eaa22cf309a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1784379865908692>:7\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m     display(df)\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n",
       "\n",
       "File \u001B[0;32m<command-1784379865908692>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n",
       "\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mc2VsZWN0IDp0ZXh0\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m   display(df)\n",
       "\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNBOUND_SQL_PARAMETER] Found the unbound parameter: state. Please, fix `args` and provide a mapping of the parameter to a SQL literal.; line 1 pos 7;\n",
       "'Project [unresolvedalias(parameter(state), None)]\n",
       "+- OneRowRelation\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1784379865908692>:7\u001B[0m\n\u001B[1;32m      5\u001B[0m     display(df)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n\nFile \u001B[0;32m<command-1784379865908692>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mc2VsZWN0IDp0ZXh0\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m   display(df)\n\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNBOUND_SQL_PARAMETER] Found the unbound parameter: state. Please, fix `args` and provide a mapping of the parameter to a SQL literal.; line 1 pos 7;\n'Project [unresolvedalias(parameter(state), None)]\n+- OneRowRelation\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNBOUND_SQL_PARAMETER] Found the unbound parameter: state. Please, fix `args` and provide a mapping of the parameter to a SQL literal.; line 1 pos 7;\n'Project [unresolvedalias(parameter(state), None)]\n+- OneRowRelation\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select :state\n",
    "\n",
    "-- not sure why this is not working in community account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49a00fd9-788e-45a1-9e8e-971770da4e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks `dbutils.notebook` Command Guide\n",
    "\n",
    "The `dbutils.notebook` utilities in Databricks are used to call one notebook from another, allowing modular pipeline design and parameter passing between notebooks.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 1. `dbutils.notebook.run()`\n",
    "- Runs a child notebook and optionally passes parameters. Execution is **synchronous**.\n",
    "\n",
    "```python\n",
    "\n",
    "dbutils.notebook.run(notebook_path: str, timeout_seconds: int, arguments: Dict[str, str]) -> str\n",
    "\n",
    "```\n",
    "---\n",
    "## Paramenters\n",
    "\n",
    "| Parameter         | Type   | Description                                                         |\n",
    "| ----------------- | ------ | ------------------------------------------------------------------- |\n",
    "| `notebook_path`   | string | Path to the child notebook (relative or absolute).                  |\n",
    "| `timeout_seconds` | int    | Maximum wait time in seconds for the notebook to finish execution.  |\n",
    "| `arguments`       | dict   | Dictionary of string key-value pairs to pass to the child notebook. |\n",
    "\n",
    "---\n",
    "## 🧩  2. %run Command in Databricks\n",
    "- %run includes and executes another notebook inline in the current notebook’s context.\n",
    "Unlike dbutils.notebook.run(), it is not used for modular pipeline control, but for reusing functions, variables, and shared code.\n",
    "\n",
    "### example \n",
    "- helper_notebook (Path: /Shared/helper_notebook)\n",
    "```python\n",
    "\n",
    "# Define a reusable function\n",
    "def greet_user(name):\n",
    "    return f\"Hello, {name} 👋\"\n",
    "\n",
    "# Define a shared variable\n",
    "default_name = \"Ayush\"\n",
    "```\n",
    "- main_notebook\n",
    "\n",
    "```python\n",
    "\n",
    "# Include the helper_notebook\n",
    "%run /Shared/helper_notebook\n",
    "\n",
    "# Now you can use greet_user and default_name directly\n",
    "print(greet_user(default_name))  # Output: Hello, Ayush 👋\n",
    "\n",
    "```\n",
    "\n",
    "| Feature                | `%run`                    | `dbutils.notebook.run()`             |\n",
    "| ---------------------- | ------------------------- | ------------------------------------ |\n",
    "| Purpose                | Share variables/functions | Run a notebook like a subprocess     |\n",
    "| Parameter Passing      | ❌ Not supported           | ✅ Yes, via `arguments`               |\n",
    "| Return Value           | ❌ None                    | ✅ Return string from child           |\n",
    "| Variable Scope Sharing | ✅ Shared with caller      | ❌ Isolated between caller and callee |\n",
    "| Execution Type         | Inline execution          | Separate, blocking call              |\n",
    "\n",
    "\n",
    "---\n",
    "## 🧩 3. dbutils.notebook.exit()\n",
    "- Used in the child notebook to return a result to the parent notebook. or stop the notebook\n",
    "```python\n",
    "\n",
    "# Notebook exit command  \n",
    "dbutils.notebook.exit(value: str)\n",
    "\n",
    "# example 1\n",
    "result_message = \"Completed loading data successfully.\"\n",
    "dbutils.notebook.exit(result_message)\n",
    "\n",
    "# example 2\n",
    "\n",
    "# run this from parent notebook)\n",
    "response = dbutils.notebook.run(\"/Users/ayush/child_job\", 120, {\"param1\": \"val1\"})\n",
    "print(\"Child notebook returned:\", response)\n",
    "\n",
    "# code in child notebook \n",
    "dbutils.widgets.text(\"param1\", \"\")\n",
    "param1_val = dbutils.widgets.get(\"param1\")\n",
    "dbutils.notebook.exit(f\"Received param1 = {param1_val}\")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1784379865908692,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_dbfs_secret_widget",
   "widgets": {
    "ComboBox": {
     "currentValue": "X",
     "nuid": "bb27721e-188c-491c-b98d-205582b09695",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "X",
      "label": "ComboBox  Widget Example",
      "name": "ComboBox",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "X",
        "Y",
        "Z"
       ],
       "fixedDomain": false,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "combobox",
      "defaultValue": "X",
      "label": "ComboBox  Widget Example",
      "name": "ComboBox",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "X",
        "Y",
        "Z"
       ]
      }
     }
    },
    "DropDown": {
     "currentValue": "A",
     "nuid": "0dfc3bcf-0e71-421d-a317-8ff96d3bebb5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "A",
      "label": "DropDown Widget Example",
      "name": "DropDown",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "A",
        "B",
        "C"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "A",
      "label": "DropDown Widget Example",
      "name": "DropDown",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "A",
        "B",
        "C"
       ]
      }
     }
    },
    "MultiSelect": {
     "currentValue": "X",
     "nuid": "86312193-f27a-4b81-a374-7e95966910e2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "X",
      "label": "MultiSelect  Widget Example",
      "name": "MultiSelect",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "X",
        "Y",
        "Z"
       ],
       "fixedDomain": true,
       "multiselect": true
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "multiselect",
      "defaultValue": "X",
      "label": "MultiSelect  Widget Example",
      "name": "MultiSelect",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "X",
        "Y",
        "Z"
       ]
      }
     }
    },
    "state": {
     "currentValue": "CA",
     "nuid": "b8b52381-659d-49ec-aafe-92a427cdccc1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "CA",
      "label": null,
      "name": "state",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "CA",
        "IL",
        "MI",
        "NY",
        "OR",
        "VA"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "CA",
      "label": null,
      "name": "state",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "CA",
        "IL",
        "MI",
        "NY",
        "OR",
        "VA"
       ]
      }
     }
    },
    "text": {
     "currentValue": "default_value",
     "nuid": "0316ac66-0024-4f86-be2c-24170a15d39e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default_value",
      "label": "Text Widget Example",
      "name": "text",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default_value",
      "label": "Text Widget Example",
      "name": "text",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
