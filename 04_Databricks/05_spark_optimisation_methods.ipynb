{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57ffa643-0fc9-443b-916a-6ac79ef09efc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "*https://www.databricks.training/spark-ui-simulator/index.html*\n",
    "\n",
    "\n",
    "## 1. Application Performance \n",
    "- The 5 most common performace problem (5's)\n",
    "  - **1. Skew** - Imbalance in the size of the partitions.\n",
    "  - **2. Spill** - Writing of Temp files to disk due to lack of memory.\n",
    "  - **3. Storage** - A set of the problems directly related to how data is stored in disk.\n",
    "  - **4. Shuffle** - Act of data moving between the executors.\n",
    "  - **5. Serialization** - Distribution code of segments accross the cluster.\n",
    "\n",
    "  ### One problem can cause others \n",
    "    - **Skew** can induce **Spill**\n",
    "    - **Storage** issue can induce excess **shuffle** \n",
    "    - Incorrectly addressed **Shuffle** can exacerbate **Skew**\n",
    "    - many of these prolems are present at same time\n",
    "\n",
    "## 2. General Optimization \n",
    "* ## 1. Table Optimization\n",
    "  - 1. Z Order\n",
    "  - 2. Data Skipping\n",
    "  - 3. Vacuum\n",
    "  - 4. Partitioning\n",
    "  - 5. Deletion Vectors\n",
    "  - 6. Liquid Clustering\n",
    "  - 7. Predictive Optimization\n",
    "\n",
    "* ## 2. Cluster Tuning\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d35c600-cd9d-4119-b775-daaffde131de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1.1 ⚖️ Data Skew in Databricks\n",
    "\n",
    "**Data skew** is a common performance bottleneck in distributed systems like Apache Spark and Databricks, where certain partitions contain significantly more data than others. This causes slow processing, inefficient resource use, and possible failures.\n",
    "\n",
    "Data skew occurs when **some keys or partitions hold much more data** than others. It leads to unbalanced work distribution across executors.\n",
    "\n",
    "### Common Causes\n",
    "\n",
    "| Cause                      | Description                                                    |\n",
    "| -------------------------- | -------------------------------------------------------------- |\n",
    "| Skewed key values          | One or few keys dominate the dataset                           |\n",
    "| NULLs                      | All NULLs grouped in one partition                             |\n",
    "| Poor partitioning          | Uneven distribution due to default hash or manual partitioning |\n",
    "| High-cardinality imbalance | Certain keys have much more data than others                   |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ Why It Matters\n",
    "\n",
    "| Issue                  | Impact                                                       |\n",
    "| ---------------------- | ------------------------------------------------------------ |\n",
    "| ⏱ Slow tasks           | Some partitions take significantly longer to compute         |\n",
    "| 🛋 Shuffle overload    | Large shuffles stress network and disk                       |\n",
    "| 📉 OOM errors          | Large partitions may cause out-of-memory failures            |\n",
    "| 🥴 Underutilized nodes | Most workers stay idle waiting for one skewed task to finish |\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 How to Detect Skew in Databricks\n",
    "\n",
    "* **Spark UI** > Stages > Tasks:\n",
    "\n",
    "  * Look for tasks with large data volume or unusually long durations\n",
    "* **Skew logs**: Spark may emit warnings for skewed joins\n",
    "* **File size patterns**: After write, some files may be much larger\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Solutions to Handle Skew\n",
    "\n",
    "### 1. **Salting the Key**\n",
    "\n",
    "Add random digits to skewed keys to distribute them across partitions.\n",
    "\n",
    "```python\n",
    "\n",
    "from pyspark.sql.functions import concat, col, lit, rand\n",
    "\n",
    "df = df.withColumn(\"salted_key\", concat(col(\"join_key\"), lit(\"_\"), (rand() * 10).cast(\"int\")))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Broadcast Join**\n",
    "\n",
    "Broadcast small dimension tables to avoid shuffling.\n",
    "\n",
    "```python\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df_large.join(broadcast(df_small), \"join_key\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Repartitioning**\n",
    "\n",
    "Repartition the DataFrame before join or shuffle.\n",
    "\n",
    "```python\n",
    "\n",
    "df.repartition(\"join_key\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Skew Join Hints (DBR 7.3+)**\n",
    "\n",
    "```sql\n",
    "\n",
    "SELECT /*+ SKEW('large_df') */ *\n",
    "FROM large_df JOIN small_df\n",
    "ON large_df.key = small_df.key;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Replace NULLs**\n",
    "\n",
    "Avoid NULL clustering by replacing them.\n",
    "\n",
    "```python\n",
    "\n",
    "df.fillna({\"join_key\": \"UNKNOWN\"})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Benefits of Skew Handling\n",
    "\n",
    "| Benefit                 | Description                                     |\n",
    "| ----------------------- | ----------------------------------------------- |\n",
    "| ⚡ Faster Jobs           | Better distribution leads to quicker completion |\n",
    "| 🚀 Fewer Failures       | Avoid OOM and executor crashes                  |\n",
    "| 📊 Balanced Load        | All cores and workers used efficiently          |\n",
    "| 🚗 Better Network Usage | Avoids unnecessary shuffle traffic              |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠ Limitations\n",
    "\n",
    "| Limitation               | Description                                         |\n",
    "| ------------------------ | --------------------------------------------------- |\n",
    "| 🔧 Salting complexity    | Requires changes to logic before and after joins    |\n",
    "| 🚫 Broadcast limitations | Only for small tables that fit in memory            |\n",
    "| 🔄 Repartition cost      | Adds shuffle overhead if misused                    |\n",
    "| ❌ Skew hint support      | Only in newer DBR versions                          |\n",
    "| ⏳ Dynamic skew           | Skew may change over time; needs regular monitoring |\n",
    "\n",
    "---\n",
    "\n",
    "## 📒 Summary\n",
    "\n",
    "| Topic     | Detail                                                  |\n",
    "| --------- | ------------------------------------------------------- |\n",
    "| Problem   | Uneven distribution of data across partitions           |\n",
    "| Impact    | Slow jobs, high memory usage, shuffle cost              |\n",
    "| Fixes     | Salting, Broadcast, Repartition, Skew Hints             |\n",
    "| Benefits  | Improved speed, fewer failures, balanced resource usage |\n",
    "| Tradeoffs | Added complexity, compute overhead, tuning required     |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca1ed8b0-6b0b-4980-bd72-5edf31b802e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1.2 📀 Spill in Databricks\n",
    "\n",
    "In **Databricks** (based on Apache Spark), **spill** refers to writing intermediate data from memory to disk when memory is insufficient during operations like joins, sorts, or aggregations. Spill is a fallback mechanism to avoid job failure, but it impacts performance.\n",
    "\n",
    "Spill happens when Spark **runs out of execution memory** and is forced to **offload data to disk** to continue processing.\n",
    "\n",
    "### Typical Scenarios:\n",
    "\n",
    "* Large **shuffle joins** or **groupBy** operations\n",
    "* **Sort** operations on big datasets\n",
    "* **Skewed partitions** or insufficient executor memory\n",
    "* Multiple wide transformations in a pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Types of Spill\n",
    "\n",
    "| Type                | Description                                             |\n",
    "| ------------------- | ------------------------------------------------------- |\n",
    "| **Shuffle Spill**   | Data spilled during shuffle operations                  |\n",
    "| **Sort Spill**      | Sort buffers that overflow memory are written to disk   |\n",
    "| **Aggregate Spill** | Hash tables used in aggregation spill to disk when full |\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 How Spark Manages Spill\n",
    "\n",
    "Spark uses `UnifiedMemoryManager` to divide JVM memory into:\n",
    "\n",
    "* **Execution memory** (joins, sorts, shuffles)\n",
    "* **Storage memory** (caching, broadcast)\n",
    "\n",
    "If execution memory exceeds its limit, **spill is triggered** to prevent `OutOfMemoryError`.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 How to Detect Spill\n",
    "\n",
    "* **Spark UI > Stages > Tasks Tab**: Look for high \"Spilled Bytes\"\n",
    "* **Task skew**: Large variance in task durations\n",
    "* **Logs**: Messages like `Spilling map output` or `Spilling sort buffer`\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Benefits of Spill\n",
    "\n",
    "| Benefit                 | Description                                         |\n",
    "| ----------------------- | --------------------------------------------------- |\n",
    "| 🛡 Prevents Job Failure | Avoids out-of-memory crashes during execution       |\n",
    "| 📀 Handles Large Data   | Processes datasets bigger than memory               |\n",
    "| ✅ Graceful Degradation  | Keeps job running at the cost of speed              |\n",
    "| 🚗 Supports Autoscaling | Complements cluster autoscaling for large workloads |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠ Limitations of Spill\n",
    "\n",
    "| Limitation                     | Explanation                                         |\n",
    "| ------------------------------ | --------------------------------------------------- |\n",
    "| ⏳ Performance Overhead         | Disk I/O is slower than memory                      |\n",
    "| ⛔ Frequent Spill = Poor Tuning | Could signal skew, bad joins, or too few partitions |\n",
    "| 💳 Increased Storage Cost      | Extra reads/writes add cloud IOPS cost              |\n",
    "| ❌ Not a True Optimization      | Spill avoids failure, not a performance tactic      |\n",
    "| 🔬 Debug Complexity            | Root cause can be hidden without proper tools       |\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ How to Minimize Spill\n",
    "\n",
    "| Strategy                        | Action                                              |\n",
    "| ------------------------------- | --------------------------------------------------- |\n",
    "| ↗ Increase Executor Memory      | Allocate more memory per executor                   |\n",
    "| 🧨 Broadcast Small Tables       | Avoid shuffle by replicating small dimension tables |\n",
    "| 🔄 Repartition                  | Balance data before wide transformations            |\n",
    "| 🌌 Fix Data Skew                | Use salting, skew join hints                        |\n",
    "| 📦 Select Relevant Columns      | Avoid loading unnecessary data                      |\n",
    "| ❄️ Use Adaptive Query Execution | Let Spark optimize joins and partitions dynamically |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙ Configuration Parameters\n",
    "\n",
    "| Parameter                      | Purpose                                              |\n",
    "| ------------------------------ | ---------------------------------------------------- |\n",
    "| `spark.memory.fraction`        | % of JVM used for storage + execution (default: 0.6) |\n",
    "| `spark.memory.storageFraction` | % of `memory.fraction` for caching (default: 0.5)    |\n",
    "| `spark.sql.shuffle.partitions` | Controls shuffle output partitions (default: 200)    |\n",
    "\n",
    "---\n",
    "\n",
    "## 📒 Summary\n",
    "\n",
    "| Feature      | Detail                                          |\n",
    "| ------------ | ----------------------------------------------- |\n",
    "| Definition   | Disk write fallback for out-of-memory scenarios |\n",
    "| Triggered By | Joins, sorts, groupBy, shuffles, skewed data    |\n",
    "| Pros         | Job safety, large data handling                 |\n",
    "| Cons         | Slower performance, expensive I/O               |\n",
    "| Detection    | Spark UI, logs, long task runtimes              |\n",
    "| Fixes        | Tune memory, repartition, use AQE, avoid skew   |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dda10fef-74a0-4b28-be5b-445b35a1e57a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1.3  📆 Storage Optimization in Databricks\n",
    "\n",
    "Storage optimization in Databricks focuses on improving **performance**, **cost-efficiency**, and **scalability** by managing how data is stored, laid out, and read in Delta Lake or Parquet format. Key strategies include partitioning, clustering, compaction, and more.\n",
    "\n",
    "---\n",
    "## 🔧 Configuration Settings\n",
    "\n",
    "| Setting                                             | Purpose                              |\n",
    "| --------------------------------------------------- | ------------------------------------ |\n",
    "| `spark.databricks.delta.optimizeWrite.enabled=true` | Merges small files during write      |\n",
    "| `spark.databricks.delta.autoCompact.enabled=true`   | Triggers auto compaction after write |\n",
    "| `spark.sql.files.maxPartitionBytes`                 | Controls partition size for reading  |\n",
    "\n",
    "---\n",
    "\n",
    "## 📅 Benefits of Storage Optimization\n",
    "\n",
    "| Benefit               | Description                                   |\n",
    "| --------------------- | --------------------------------------------- |\n",
    "| ⚡ Faster Queries      | Reduces data scanned via pruning and skipping |\n",
    "| 💸 Cost Efficiency    | Minimizes storage and compute cost            |\n",
    "| 💡 Better Performance | Less memory, I/O, and shuffle overhead        |\n",
    "| 🚀 Scalability        | Enables petabyte-scale operations             |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠ Limitations and Considerations\n",
    "\n",
    "| Limitation                        | Explanation                                           |\n",
    "| --------------------------------- | ----------------------------------------------------- |\n",
    "| ⏳ OPTIMIZE is expensive           | Full-table rewrites can be costly                     |\n",
    "| 📉 Partition skew                 | Too many or too few partitions can reduce performance |\n",
    "| ❌ Z-ORDER is static               | Doesn't auto-adapt to workload changes                |\n",
    "| 🔹 Temporary overhead             | Optimizations may temporarily use more disk           |\n",
    "| ⚡ VACUUM removes time travel data | Irreversible deletion of old files                    |\n",
    "\n",
    "---\n",
    "\n",
    "## 📒 Summary\n",
    "\n",
    "| Technique         | Purpose                     | Best Use Case                    | SQL Code                           |\n",
    "| ----------------- | --------------------------- | -------------------------------- |--------------------------------\n",
    "| Partitioning      | Skip large data segments    | Filters on date, region          | CREATE TABLE sales PARTITIONED BY (region, date); |\n",
    "| Z-Ordering        | Improve file block locality | Joins/filters on key columns     | OPTIMIZE sales ZORDER BY (customer_id, date); |\n",
    "| Liquid Clustering | Auto file layout            | Real-time or streaming workloads | ALTER TABLE events SET CLUSTER BY (event_type, date); |\n",
    "| File Compaction   | Fix small files             | Frequent small writes            | OPTIMIZE table_name; |\n",
    "| VACUUM            | Clean unused data           | Save storage cost                | VACUUM table_name RETAIN 168 HOURS; |\n",
    "| Data Skipping     | Auto filter out blocks      | Enabled by default on Delta      | |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ab823e2-89a6-4d79-9fcb-1e0bf6489ebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.4 🔄 Databricks Shuffle Optimization – Complete Guide\n",
    "\n",
    "Shuffle is a critical part of Spark and Databricks jobs, especially for wide transformations (like joins, groupBy, distinct, orderBy). However, it’s also expensive — it involves disk I/O, network I/O, and memory usage. Optimizing shuffle helps reduce cost, improve job performance, and prevent failures.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ What is Shuffle?\n",
    "\n",
    "Shuffle happens when Spark redistributes data across partitions — usually triggered by:\n",
    "\n",
    "* `groupByKey`, `reduceByKey`, `join`, `distinct`, `repartition`, `orderBy`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Key Strategies for Shuffle Optimization\n",
    "\n",
    "### 1. **Use Narrow Transformations Where Possible**\n",
    "\n",
    "* Prefer `map`, `filter`, `flatMap`, `mapPartitions` — these don’t trigger shuffle.\n",
    "\n",
    "### 2. **Use `reduceByKey` Instead of `groupByKey`**\n",
    "\n",
    "* `reduceByKey` does a map-side combine before shuffle, reducing the amount of data shuffled.\n",
    "\n",
    "### 3. **Broadcast Join Optimization**\n",
    "\n",
    "Use **broadcast join** when one table is small enough (< 10 GB typically):\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "df1.join(broadcast(df2), \"id\")\n",
    "```\n",
    "\n",
    "* Reduces shuffle by sending the smaller dataset to all executors.\n",
    "\n",
    "### 4. **Use Adaptive Query Execution (AQE)**\n",
    "\n",
    "* AQE dynamically optimizes joins and shuffles at runtime.\n",
    "  Enable in Spark config:\n",
    "\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "Key AQE features:\n",
    "\n",
    "* Dynamic shuffle partitioning\n",
    "* Skew join handling\n",
    "* Dynamically switching join strategies\n",
    "\n",
    "### 5. **Optimize Number of Shuffle Partitions**\n",
    "\n",
    "Default: `spark.sql.shuffle.partitions = 200`\n",
    "\n",
    "* Lower it for small jobs, raise for large ones.\n",
    "* Tune based on dataset size, number of executors, and partitions.\n",
    "\n",
    "### 6. **Handle Skewed Data**\n",
    "\n",
    "Skew causes some tasks to process more data than others.\n",
    "\n",
    "#### Strategies:\n",
    "\n",
    "* Use **salting** for skewed keys\n",
    "* Use **AQE's skew join handling**\n",
    "* Filter hot keys into a separate branch\n",
    "\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "### 7. **Avoid Unnecessary `repartition` and `coalesce`**\n",
    "\n",
    "* `repartition()` causes full shuffle — avoid unless needed.\n",
    "* Use `coalesce()` to reduce partitions **without** full shuffle.\n",
    "\n",
    "### 8. **Use Z-Ordering with Delta Tables**\n",
    "\n",
    "* Helps skip files efficiently and improves read performance after shuffle-heavy operations.\n",
    "\n",
    "```sql\n",
    "OPTIMIZE table_name ZORDER BY (col1, col2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Useful Configs for Shuffle Optimization\n",
    "\n",
    "| Config Key                                      | Description                   | Recommended                                |\n",
    "| ----------------------------------------------- | ----------------------------- | ------------------------------------------ |\n",
    "| `spark.sql.shuffle.partitions`                  | Default shuffle partitions    | Tune based on workload (e.g. 200–1000)     |\n",
    "| `spark.sql.adaptive.enabled`                    | Enables AQE                   | `true`                                     |\n",
    "| `spark.sql.adaptive.skewJoin.enabled`           | Handles skew joins            | `true`                                     |\n",
    "| `spark.sql.adaptive.coalescePartitions.enabled` | Dynamically reduce partitions | `true`                                     |\n",
    "| `spark.sql.autoBroadcastJoinThreshold`          | Size threshold for broadcast  | Set based on memory (e.g., `10MB`, `50MB`) |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Best Practices Checklist\n",
    "\n",
    "* [x] Enable AQE\n",
    "* [x] Broadcast small lookup tables\n",
    "* [x] Tune `shuffle.partitions` per job size\n",
    "* [x] Use `reduceByKey` instead of `groupByKey`\n",
    "* [x] Avoid wide transformations unless necessary\n",
    "* [x] Handle skewed keys carefully\n",
    "* [x] Repartition only when really needed\n",
    "* [x] Monitor job DAG & metrics from Spark UI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e46744c-c55e-4a8d-9bbe-918c763558b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1.5 🔄 Serialization in Databricks\n",
    "\n",
    "**Serialization** in Databricks refers to the process of converting **in-memory objects (like DataFrames, RDDs, or objects)** into a **byte stream** for transmission across nodes or writing to disk. It is essential for distributed computing and affects performance, memory, and network efficiency.\n",
    "\n",
    "Serialization is:\n",
    "\n",
    "> 🛠️ **Converting structured objects into a storable or transmittable format**, later reconstructed via deserialization.\n",
    "\n",
    "### Used In:\n",
    "\n",
    "* Shuffle (e.g., joins, groupBy)\n",
    "* Caching or persisting\n",
    "* Writing files (Delta, Parquet, JSON)\n",
    "* UDF processing\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Serialization Formats in Databricks\n",
    "\n",
    "| Format                 | Description                                                |\n",
    "| ---------------------- | ---------------------------------------------------------- |\n",
    "| **Java Serialization** | Default; flexible but slow and bulky                       |\n",
    "| **Kryo Serialization** | Compact and fast; requires registration for custom classes |\n",
    "| **Parquet/ORC/Avro**   | Used for storage; columnar and efficient                   |\n",
    "| **Delta**              | Layer on top of Parquet with ACID metadata                 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Where Serialization Happens\n",
    "\n",
    "* During **shuffle** (joins, aggregations)\n",
    "* **Caching** with `.cache()` or `.persist()`\n",
    "* Writing to storage formats (Parquet, Delta, etc.)\n",
    "* Passing objects in **UDFs** across executors\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Benefits of Serialization\n",
    "\n",
    "| Benefit                       | Description                                      |\n",
    "| ----------------------------- | ------------------------------------------------ |\n",
    "| ⚡ Faster Network Transfer     | Smaller, compact byte streams transmitted faster |\n",
    "| 📀 Efficient Storage          | Parquet/Kryo saves disk/memory space             |\n",
    "| 🧠 Memory Management          | Kryo reduces object overhead in JVM heap         |\n",
    "| 🚀 Better Shuffle Performance | Serialization reduces I/O bottlenecks            |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Limitations of Serialization\n",
    "\n",
    "| Limitation                    | Description                                          |\n",
    "| ----------------------------- | ---------------------------------------------------- |\n",
    "| ⏳ Java Serialization is slow  | Inefficient for large workloads                      |\n",
    "| 🔧 Kryo requires setup        | Complex objects must be registered manually          |\n",
    "| 👾 Opaque Format              | Not human-readable; needs deserialization to inspect |\n",
    "| 🔄 (De)serialization Overhead | Adds latency for large/complex objects               |\n",
    "| ⚡ Compatibility Risks         | Schema or code changes may break deserialization     |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Enabling Kryo in Databricks\n",
    "\n",
    "```python\n",
    "\n",
    "spark.conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "spark.conf.set(\"spark.kryo.registrationRequired\", \"false\")\n",
    "```\n",
    "\n",
    "Register custom classes:\n",
    "\n",
    "```python\n",
    "\n",
    "spark.conf.set(\"spark.kryo.classesToRegister\", \"com.mycompany.MyClass\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📄 Summary\n",
    "\n",
    "| Topic       | Detail                                                  |\n",
    "| ----------- | ------------------------------------------------------- |\n",
    "| Purpose     | Convert memory objects to bytes for I/O or distribution |\n",
    "| Common Uses | Shuffle, caching, UDFs, file writes                     |\n",
    "| Formats     | Java, Kryo, Parquet, Delta                              |\n",
    "| Benefits    | Faster transfer, memory efficiency, better performance  |\n",
    "| Limitations | Overhead, complexity, format compatibility              |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91df9d09-bb75-4b4f-a1fd-bb948768daac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2.1.1 🔍 Z-Ordering\n",
    "- Z-Ordering (or Z-Indexing) is a multi-dimensional clustering technique that reorders data within files to optimize data skipping during queries.\n",
    "\n",
    "---\n",
    "```sql\n",
    "\n",
    "OPTIMIZE delta.`/path/to/table`\n",
    "ZORDER BY (customer_id, order_date);\n",
    "\n",
    "OPTIMIZE my_catalog.sales.orders\n",
    "ZORDER BY (customer_id, order_date);\n",
    "```\n",
    "---\n",
    "\n",
    "## 🔍 Behind the Scenes: What Happens?\n",
    "  - Compaction: OPTIMIZE merges many small files into fewer large files.\n",
    "  - Z-ordering: Within each new file, records are re-ordered using Z-curve logic based on chosen columns.\n",
    "  - Data Skipping: During queries, Spark looks at min/max stats for each file → skips files that don’t match the filter.\n",
    "\n",
    "## 🔎 Performance Gains\n",
    "  - Query filtering on Z-ordered columns can skip up to 90–98% of files.\n",
    "  - Major speedups observed for high-cardinality columns with selective filtering.\n",
    "\n",
    "## 📌 When to Use Z-Ordering\n",
    "| Use Case                                | Z-Ordering Recommended?              |\n",
    "| --------------------------------------- | ------------------------------------ |\n",
    "| Filtering on non-partitioned columns    | ✅ Yes                                |\n",
    "| Frequent range scans                    | ✅ Yes                                |\n",
    "| Filtering on booleans (low cardinality) | ❌ Not helpful                        |\n",
    "| Full table scans                        | ❌ No benefit                         |\n",
    "| Streaming workloads                     | ❌ Z-order not supported in streaming |\n",
    "| ❌ Not for streaming tables             |\tMust be batch   |\n",
    "\n",
    "## 🧹 Schedule Z-Ordering\n",
    "- You can schedule OPTIMIZE ZORDER in a notebook or workflow:\n",
    "```sql\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "OPTIMIZE main.retail.transactions\n",
    "ZORDER BY (customer_id, txn_date)\n",
    "\"\"\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ad65719-ff14-449d-83d8-0374cc38b85c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2.1.2 🔍 Data Skipping in Databricks – Complete Guide\n",
    "\n",
    "**Data Skipping** is a performance optimization feature in Databricks (Delta Lake) that allows Spark to **avoid scanning unnecessary files** during query execution by leveraging **metadata-level statistics**.\n",
    "\n",
    "Data Skipping allows Delta Lake to skip reading files that **cannot possibly match the query filters**, based on per-file statistics like min/max values for each column.\n",
    "\n",
    "> Instead of reading all files, Spark checks the Delta log metadata to see if a file's data range overlaps with the query filter.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ How Data Skipping Works\n",
    "\n",
    "1. Delta Lake stores **file-level statistics** (minValue, maxValue, nullCount) in the Delta transaction log.\n",
    "2. When a query with a filter is issued, Spark compares the filter condition with these statistics.\n",
    "3. Files that **do not match the filter** are **skipped**, saving I/O and improving performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Example\n",
    "\n",
    "If a Delta table contains 1000 files and only 1 file contains rows for `date = '2023-01-01'`,\n",
    "\n",
    "```sql\n",
    "SELECT * FROM sales WHERE date = '2023-01-01'\n",
    "```\n",
    "\n",
    "will **only scan that 1 file**, skipping the other 999.\n",
    "\n",
    "---\n",
    "\n",
    "## 👁 Statistics Used\n",
    "\n",
    "| Statistic   | Description                                |\n",
    "| ----------- | ------------------------------------------ |\n",
    "| `minValue`  | Minimum value of a column in a file        |\n",
    "| `maxValue`  | Maximum value of a column in a file        |\n",
    "| `nullCount` | Number of null values per column in a file |\n",
    "\n",
    "These are automatically collected during write, update, and merge operations.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 How to Enable or Verify\n",
    "\n",
    "* Data Skipping is **enabled by default** on all Delta tables.\n",
    "* To verify:\n",
    "\n",
    "```sql\n",
    "DESCRIBE DETAIL delta.`/path/to/table`\n",
    "```\n",
    "\n",
    "* Or inspect Delta logs under `_delta_log/` for statistics in commit JSON files.\n",
    "\n",
    "---\n",
    "\n",
    "## 📅 Benefits of Data Skipping\n",
    "\n",
    "| Benefit              | Description                                 |\n",
    "| -------------------- | ------------------------------------------- |\n",
    "| ⚡ Faster Queries     | Files not matching filters are not read     |\n",
    "| 💸 Cost Efficient    | Saves compute, I/O, and memory resources    |\n",
    "| 🔓 Easy to Use       | No manual config needed                     |\n",
    "| ✨ Scales with ZORDER | Even more efficient with sorted file layout |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠ Limitations of Data Skipping\n",
    "\n",
    "| Limitation                           | Explanation                                       |\n",
    "| ------------------------------------ | ------------------------------------------------- |\n",
    "| 🛠️ Delta-only                       | Only works on Delta tables                        |\n",
    "| ❌ Not helpful for full scans         | Skipping is bypassed when scanning entire table   |\n",
    "| 🥺 Inaccurate stats = no skipping    | Non-standard writes may skip stats collection     |\n",
    "| 🚫 No benefit for unstructured types | Works best on integers, strings, dates            |\n",
    "| ⚡ Update-heavy workloads             | Require re-OPTIMIZE to refresh effective skipping |\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ How to Maximize Skipping\n",
    "\n",
    "| Strategy                    | Action                                          |\n",
    "| --------------------------- | ----------------------------------------------- |\n",
    "| Partition wisely            | Use on low-cardinality, filter-heavy columns    |\n",
    "| Use `OPTIMIZE ZORDER`       | Physically co-locate similar values             |\n",
    "| Avoid UDF filters           | Use simple `WHERE column = 'value'` expressions |\n",
    "| Stick to standard writes    | Use DataFrame APIs, `MERGE`, `COPY INTO`, etc.  |\n",
    "| Periodically run `OPTIMIZE` | Rewrites files for efficient skipping           |\n",
    "\n",
    "---\n",
    "\n",
    "## 📒 Summary\n",
    "\n",
    "| Feature     | Description                                          |\n",
    "| ----------- | ---------------------------------------------------- |\n",
    "| What        | Skips reading files that can't satisfy query filters |\n",
    "| How         | Uses per-file column stats from Delta logs           |\n",
    "| When        | Filtered queries on Delta tables                     |\n",
    "| Benefits    | Faster, cheaper queries without manual tuning        |\n",
    "| Limitations | Not for full scans, unstructured data, or non-Delta  |\n",
    "| Tips        | Use ZORDER, avoid skew, use simple filters           |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "903d6254-4255-491d-93ff-4f2942971eaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2.1.3🧹 VACUUM in Databricks – Complete Guide\n",
    "\n",
    "The `VACUUM` command in Databricks is used to clean up **old, unreferenced data files** from Delta Lake tables. It's essential for **managing storage**, **cost**, and **performance** in your Lakehouse environment.\n",
    "\n",
    "In Delta Lake, every time you update, delete, or overwrite data, **new Parquet files** are created while the **old files are retained** to support **ACID transactions**, **time travel**, and **rollback**.\n",
    "\n",
    "`VACUUM` removes these **obsolete files** that are **no longer needed** by Delta Lake.\n",
    "\n",
    "```sql\n",
    "\n",
    "VACUUM delta_table_name RETAIN 168 HOURS;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 How It Works\n",
    "\n",
    "* **Delta transaction log** tracks all file versions.\n",
    "* `VACUUM` uses this log to find files **not referenced** by the current table state or by any time travel version within the retention period.\n",
    "* It **physically deletes** these files from storage.\n",
    "\n",
    "---\n",
    "\n",
    "## 🕒 Retention Period\n",
    "\n",
    "Delta Lake **defaults to 7 days (168 hours)** to ensure safe time travel and rollback.\n",
    "\n",
    "You can **change this retention**:\n",
    "\n",
    "```sql  \n",
    "\n",
    "VACUUM delta_table_name RETAIN 24 HOURS; -- 1 day\n",
    "```\n",
    "\n",
    "But note:\n",
    "\n",
    "> ⚠️ **Shorter than 7 days** requires:\n",
    "\n",
    "```sql\n",
    "\n",
    "SET spark.databricks.delta.retentionDurationCheck.enabled = false;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Syntax\n",
    "\n",
    "```sql\n",
    "\n",
    "VACUUM [db_name.]table_name [RETAIN num HOURS]\n",
    "```\n",
    "\n",
    "* `RETAIN` is optional (default = 168 hours)\n",
    "* You can also run it using PySpark:\n",
    "\n",
    "```python\n",
    "\n",
    "spark.sql(\"VACUUM my_table RETAIN 24 HOURS\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📆 Use Cases for `VACUUM`\n",
    "\n",
    "| Use Case                  | Why Important?                               |\n",
    "| ------------------------- | -------------------------------------------- |\n",
    "| Frequent updates/deletes  | Old files accumulate, need cleanup           |\n",
    "| Save storage cost         | Deleted/overwritten files still occupy space |\n",
    "| Improve performance       | Reduces unnecessary file scan                |\n",
    "| Time travel window passed | Files can now be safely deleted              |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Best Practices\n",
    "\n",
    "* ✅ Schedule periodic `VACUUM` (e.g. daily or weekly) via workflows or jobs.\n",
    "* ✅ Run after heavy overwrite, `MERGE`, or `DELETE` operations.\n",
    "* ⚠️ Be cautious reducing retention if you rely on **time travel** or **rollback**.\n",
    "* ✅ Combine with `OPTIMIZE` for storage + query performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Related Configurations\n",
    "\n",
    "| Config                                                  | Description                                    |\n",
    "| ------------------------------------------------------- | ---------------------------------------------- |\n",
    "| `spark.databricks.delta.retentionDurationCheck.enabled` | If `true`, blocks unsafe `VACUUM` under 7 days |\n",
    "| `spark.databricks.delta.vacuum.parallelDelete.enabled`  | Enables parallel deletion for faster cleanup   |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔭 Check Before Vacuuming\n",
    "\n",
    "You can list files marked for deletion (dry run) with:\n",
    "\n",
    "```python\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "deltaTable = DeltaTable.forName(spark, \"my_table\")\n",
    "df = deltaTable.vacuum(retentionHours=24, dryRun=True)\n",
    "df.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🚮 What VACUUM Does **NOT** Do\n",
    "\n",
    "* ❌ It does **not** clean up transaction logs (`_delta_log`)\n",
    "* ❌ It does **not** optimize small files — use `OPTIMIZE` for that\n",
    "* ❌ It does **not** reclaim space from the metadata store (external systems)\n",
    "\n",
    "---\n",
    "\n",
    "## 📏 Example Workflow\n",
    "\n",
    "```sql\n",
    "\n",
    "-- Optional: disable retention check for aggressive cleanup\n",
    "SET spark.databricks.delta.retentionDurationCheck.enabled = false;\n",
    "\n",
    "-- Perform cleanup\n",
    "VACUUM my_schema.my_table RETAIN 24 HOURS;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "| Feature           | Description                                |\n",
    "| ----------------- | ------------------------------------------ |\n",
    "| Purpose           | Remove obsolete Delta data files           |\n",
    "| Default Retention | 168 hours (7 days)                         |\n",
    "| Usage             | `VACUUM table [RETAIN x HOURS]`            |\n",
    "| Danger            | Reducing retention can affect time travel  |\n",
    "| Best Practice     | Schedule regularly + combine with OPTIMIZE |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "328580a4-1613-416c-bb45-fe03878f0512",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2.1.4 📂 Partitioning in Databricks\n",
    "\n",
    "**Partitioning** in Databricks (and Apache Spark/Delta Lake) refers to the **physical organization of data** into subdirectories based on one or more column values. It optimizes query performance, enables data skipping, and improves manageability for large datasets.\n",
    "\n",
    "Partitioning is the technique of **dividing data into logical chunks** stored in separate folders by partition column(s).\n",
    "\n",
    "Example:\n",
    "```\n",
    "\n",
    "/sales_data/date=2024-01-01/\n",
    "/sales_data/date=2024-01-02/\n",
    "```\n",
    "\n",
    "Each partition folder contains data specific to the partition key value.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ How to Partition in Databricks\n",
    "\n",
    "### SQL Table Creation\n",
    "\n",
    "```sql\n",
    "\n",
    "CREATE TABLE sales (\n",
    "  id STRING,\n",
    "  amount DOUBLE,\n",
    "  date STRING\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (date);\n",
    "```\n",
    "\n",
    "### DataFrame Write\n",
    "\n",
    "```python\n",
    "\n",
    "df.write.partitionBy(\"date\").format(\"delta\").save(\"/mnt/sales_data\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Use Cases for Partitioning\n",
    "\n",
    "| Use Case                 | Why It Helps                                                 |\n",
    "| ------------------------ | ------------------------------------------------------------ |\n",
    "| 🔍 Time-based analytics  | Partition by `date` or `month` for faster time range scans   |\n",
    "| 📦 Geographic queries    | Partition by `region` or `state` to isolate reads            |\n",
    "| 🔄 Incremental loads     | Enables efficient overwrite or update of specific partitions |\n",
    "| 🧪 Filtering performance | Leverages partition pruning to skip irrelevant data          |\n",
    "| ⏫ Large datasets         | Enables high parallelism for processing                      |\n",
    "\n",
    "---\n",
    "\n",
    "## 📅 Benefits of Partitioning\n",
    "\n",
    "| Benefit                  | Description                                    |\n",
    "| ------------------------ | ---------------------------------------------- |\n",
    "| ⚡ Faster Queries         | Uses partition pruning to skip unneeded files  |\n",
    "| 💸 Cost Savings          | Scans only relevant partitions, saving compute |\n",
    "| 🧱 Efficient Maintenance | Easier to rewrite/delete small data segments   |\n",
    "| 😄 Better Parallelism    | Improves multi-threaded read/write throughput  |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Limitations of Partitioning\n",
    "\n",
    "| Limitation                 | Explanation                                                    |\n",
    "| -------------------------- | -------------------------------------------------------------- |\n",
    "| 🚧 Too many partitions     | Can lead to small file issues (file explosion)                 |\n",
    "| 📉 Skewed partitions       | Uneven partition sizes cause performance imbalance             |\n",
    "| ❌ Not filter-effective     | Partition pruning works only with filters on partition columns |\n",
    "| 🔄 Repartitioning overhead | Expensive to change partition structure after write            |\n",
    "| ❎ Fixed schema             | Partitioning schema is hard to modify post-creation            |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Best Practices\n",
    "\n",
    "* ✅ Use **low-cardinality**, frequently filtered columns like `date`, `state`\n",
    "* ❌ Avoid high-cardinality fields (e.g., `user_id`, `transaction_id`)\n",
    "* ✅ Monitor for skew and small file growth\n",
    "* ✨ Combine with **Z-Ordering** or **Liquid Clustering** for better optimization\n",
    "\n",
    "---\n",
    "\n",
    "## 📄 Summary\n",
    "\n",
    "| Category         | Detail                                                        |\n",
    "| ---------------- | ------------------------------------------------------------- |\n",
    "| Purpose          | Organize data into directories by key column(s)               |\n",
    "| Example          | Partition by `date`, `region`, `state`                        |\n",
    "| Benefits         | Faster queries, parallelism, data skipping                    |\n",
    "| Limitations      | File explosion, skew, fixed layout                            |\n",
    "| Use Cases        | Time-based reporting, regional analytics, incremental updates |\n",
    "| Best Paired With | Delta Lake, OPTIMIZE, Z-ORDER, Liquid Clustering              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef53761e-acbf-45cf-ab00-64adf8c008bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2.1.5  Deletion Vectors in Delta Lake (Databricks)\n",
    "\n",
    "**Deletion Vectors (DVs)** are a feature introduced in **Delta Lake 3.0+** that allow DELETE, UPDATE, and MERGE operations to be performed **without rewriting physical Parquet files**. Instead, DVs track deleted rows in compact metadata files, improving performance and reducing cost.\n",
    "\n",
    "A **Deletion Vector** is a **bitmap index** that records **which rows in a Parquet file are logically deleted**, without physically removing the data.\n",
    "\n",
    "### Instead of:\n",
    "\n",
    "* Rewriting full files to apply deletes...\n",
    "\n",
    "### Delta Lake:\n",
    "\n",
    "* Writes a **sidecar deletion vector file** to track the deleted rows.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 How It Works\n",
    "\n",
    "1. Delta Lake stores data in Parquet files.\n",
    "2. When a row is deleted (via DELETE, MERGE, etc.):\n",
    "\n",
    "   * Delta logs the deleted row **position** in a deletion vector.\n",
    "3. During reads:\n",
    "\n",
    "   * Delta applies the deletion vector to **filter out** the deleted rows.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Benefits of Deletion Vectors\n",
    "\n",
    "| Benefit                        | Description                                                               |\n",
    "| ------------------------------ | ------------------------------------------------------------------------- |\n",
    "| ⚡ **Faster DML**               | DELETE, UPDATE, MERGE operations are much faster without rewriting files. |\n",
    "| 💸 **Lower Cost**              | Reduces storage and compute usage.                                        |\n",
    "| ♻️ **Efficient Compaction**    | Avoids file explosion due to frequent DML.                                |\n",
    "| 📖 **Better Read Performance** | When combined with Photon or vectorized reads.                            |\n",
    "| 💡 **Fine-Grained Deletes**    | Enables deleting specific rows in large files.                            |\n",
    "\n",
    "---\n",
    "\n",
    "## ❌ Limitations / Considerations\n",
    "\n",
    "| Limitation                     | Description                                                                     |\n",
    "| ------------------------------ | ------------------------------------------------------------------------------- |\n",
    "| 🔍 **Extra Metadata**          | Slight metadata overhead to store DVs.                                          |\n",
    "| 🦜 **Needs Cleanup**           | Deleted rows still exist physically — requires `OPTIMIZE` or `VACUUM` to purge. |\n",
    "| ⚡ **Requires Delta Lake 3.0+** | Not supported in older versions.                                                |\n",
    "| 🔢 **Read Impact**             | Slight read penalty (mitigated by Photon).                                      |\n",
    "| ⚖️ **Compatibility**           | Not all tools can read DV-enabled tables.                                       |\n",
    "\n",
    "---\n",
    "\n",
    "## 📗 When Are DVs Used?\n",
    "\n",
    "* Used during:\n",
    "\n",
    "  * `DELETE`\n",
    "  * `UPDATE`\n",
    "  * `MERGE`\n",
    "  * `COPY INTO`\n",
    "* If:\n",
    "\n",
    "  * Delta Lake version ≥ 3.0\n",
    "  * Runtime supports DVs (e.g., DBR 13+)\n",
    "  * Table has DVs enabled\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Enable Deletion Vectors\n",
    "\n",
    "On table level:\n",
    "\n",
    "```sql\n",
    "\n",
    "ALTER TABLE my_table SET TBLPROPERTIES (\n",
    "  'delta.enableDeletionVectors' = 'true'\n",
    ");\n",
    "```\n",
    "\n",
    "Globally:\n",
    "\n",
    "```python\n",
    "\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.enableDeletionVectors\", \"true\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔢 Check for Deletion Vectors\n",
    "\n",
    "```sql\n",
    "\n",
    "DESCRIBE DETAIL my_table;\n",
    "```\n",
    "\n",
    "Look for:\n",
    "\n",
    "* `\"deletionVectorEnabled\": true`\n",
    "* `\"numDeletionVectors\"` in the output\n",
    "\n",
    "---\n",
    "\n",
    "## 🧹 Cleanup (Physical Deletion)\n",
    "\n",
    "Use `OPTIMIZE` to remove physically deleted rows:\n",
    "\n",
    "```sql\n",
    "OPTIMIZE my_table;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📅 Summary\n",
    "\n",
    "| Feature        | Value                                             |\n",
    "| -------------- | ------------------------------------------------- |\n",
    "| Purpose        | Logical deletion without rewriting files          |\n",
    "| Introduced In  | Delta Lake 3.0                                    |\n",
    "| Benefits       | Faster DML, lower storage cost, avoids file churn |\n",
    "| Requires       | Delta Lake 3.0+, DBR 13+                          |\n",
    "| Cleanup Method | `OPTIMIZE`, `VACUUM`                              |\n",
    "| Applies To     | DELETE, UPDATE, MERGE, COPY INTO                  |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7ec6bdd-ee44-47f7-9404-1204cc7ccd29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2.1.6 🧠 What is Liquid Clustering? \n",
    "- Liquid clustering replaces table **partitioning and ZORDER** to simplify data layout decisions and optimize query performance. It provides the flexibility to redefine **clustering keys** without rewriting existing data, allowing data layout to evolve alongside analytic needs over time. Liquid clustering applies to both **Streaming Tables and Materialized Views**.\n",
    "\n",
    "  - Tables often filtered by **high cardinality** columns.\n",
    "  - Tables with significant **skew** in data distribution.\n",
    "  - Tables that **grow quickly** and require maintenance and tuning effort.\n",
    "  - Tables with **concurrent write requirements**.\n",
    "  - Tables with **access patterns** that change over time.\n",
    "  - Tables where a typical **partition key** could leave the table with too many or too few partitions.\n",
    "\n",
    "## ✅ Benefits of Liquid Clustering\n",
    "\n",
    "| Benefit                           | Description                                                                                             |\n",
    "| --------------------------------- | ------------------------------------------------------------------------------------------------------- |\n",
    "| ⚡ **Faster Queries**              | Improves read performance by reducing the amount of data scanned (especially for filter-heavy queries). |\n",
    "| 👮 **Incremental Optimization**   | Works during normal writes; no need for expensive `OPTIMIZE ZORDER`.                                    |\n",
    "| 💸 **Lower Cost**                 | Reduces the need for costly maintenance operations like full rewrites.                                  |\n",
    "| 🔀 **Automatic Management**       | No manual triggers required once configured.                                                            |\n",
    "| 🧱 **Better Small File Handling** | Maintains file size balance, reducing file explosion.                                                   |\n",
    "| 🎯 **Query-Adaptive Clustering**  | Can adjust based on query workloads (in upcoming versions).    \n",
    "---\n",
    "## ⚖️ How to Enable Liquid Clustering\n",
    "\n",
    "You define the clustering columns at table creation or alteration time.\n",
    "\n",
    "```sql\n",
    "\n",
    "CREATE TABLE my_table (\n",
    "  id STRING,\n",
    "  ts TIMESTAMP,\n",
    "  value STRING\n",
    ")\n",
    "USING DELTA\n",
    "CLUSTER BY (id, ts);\n",
    "```\n",
    "\n",
    "Or update an existing table:\n",
    "\n",
    "```sql\n",
    "\n",
    "ALTER TABLE my_table SET CLUSTER BY (id, ts);\n",
    "```\n",
    "\n",
    "Then Databricks **automatically applies clustering during writes**.\n",
    "---\n",
    "## 🚧 Limitations of Liquid Clustering\n",
    "\n",
    "| Limitation                             | Explanation                                                                     |\n",
    "| -------------------------------------- | ------------------------------------------------------------------------------- |\n",
    "| 🔄 **Only for Delta Tables**           | Requires Delta Lake format                                                      |\n",
    "| 🥺 **Still Evolving**                  | Not as mature as Z-Ordering; performance tuning is ongoing                      |\n",
    "| 💸 **Less Control**                    | You don’t get precise control over clustering logic like with manual ZORDER     |\n",
    "| 🗓️ **No Backward Optimization**       | It doesn’t re-cluster existing data immediately; only new/modified rows benefit |\n",
    "| 📦 **Requires Unity Catalog (for GA)** | Generally available only with Unity Catalog on certain runtimes                 |\n",
    "| ⚙️ **Not Available in All Runtimes**   | Requires specific Databricks runtime versions (DBR 13+ or later)                |\n",
    "---\n",
    "## 🧹 Liquid Clustering vs. Z-Ordering\n",
    "\n",
    "| Feature      | Liquid Clustering                  | Z-Ordering                           |\n",
    "| ------------ | ---------------------------------- | ------------------------------------ |\n",
    "| Trigger      | Automatic                          | Manual (`OPTIMIZE ZORDER`)           |\n",
    "| Cost         | Low (incremental)                  | High (full rewrite)                  |\n",
    "| Maintenance  | Minimal                            | Requires periodic optimization       |\n",
    "| Control      | Less granular                      | More customizable                    |\n",
    "| File Rewrite | Partial/incremental                | Full                                 |\n",
    "| Use Case     | Frequent writes, real-time updates | Mostly static or batch-loaded tables |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7277686e-1917-4679-b807-13e47e6f1e30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2.1.7 🚀 Predictive Optimization\n",
    "- A fully automated, intelligent maintenance feature for **Unity Catalog managed Delta tables**. It uses **ML to schedule and execute key optimizations** when they are most beneficial, eliminating manual planning. **Serverless compting** should be enable.\n",
    "  - OPTIMIZE (incremental clustering & compaction)\n",
    "  - ANALYZE (gathering statistics)\n",
    "  - VACUUM (removing stale files)\n",
    "---\n",
    "## 🛠️ How It Works\n",
    "- Databricks monitors your query history.\n",
    "- A ML model predicts the best data layout for performance:\n",
    "   - File sizes\n",
    "   - Data clustering (Z-order like)\n",
    "   - Column access patterns\n",
    "- The system automatically rewrites the table layout in the background.\n",
    "---\n",
    "## ✅ How to Enable Predictive Optimization\n",
    "- It is enabled by default on Unity Catalog Delta tables (Premium and Enterprise tiers).\n",
    "- **We can enable and disable at catalog level as well**\n",
    "- **Predictive Optimization -**\tENABLE (inherited from METASTORE metastore_azure_eastus)\n",
    "```sql \n",
    " -- if you want to do via manual query \n",
    "\n",
    " ALTER TABLE catalog.schema.table\n",
    "SET TBLPROPERTIES (\n",
    "  'delta.autoPredictiveOptimization' = 'true'\n",
    ");\n",
    "\n",
    "-- system table \n",
    "select * from system.storage.predictive_optimization_operations_history;\n",
    "```\n",
    "---\n",
    "## Limitation \n",
    "- Predictive optimization is not available in all regions.\n",
    "- For tables with deleted file retention duration (delta.deletedFileRetentionDuration) configured below the default of 7 days, predictive optimization runs VACUUM with retention duration as 7 days. See Configure data retention for time travel queries.\n",
    "- Predictive optimization does not perform maintenance operations on the following tables:\n",
    "\t- Tables loaded to a workspace as Delta Sharing recipients.\n",
    "\t- External tables.\n",
    "\n",
    "--- \n",
    "## 🆚 Predictive Optimization vs Manual OPTIMIZE\n",
    "| Feature                | Predictive Optimization    | Manual OPTIMIZE       |\n",
    "| ---------------------- | -------------------------- | --------------------- |\n",
    "| Trigger                | Automatic (based on usage) | Manual or scheduled   |\n",
    "| File Compaction        | ✅ Yes                      | ✅ Yes                 |\n",
    "| Z-Ordering             | ✅ Similar behavior         | ✅ Explicit            |\n",
    "| Resource Usage Control | ✅ Adaptive                 | ❌ Needs manual tuning |\n",
    "| Granularity of Control | ❌ No direct control        | ✅ Full control        |\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e631dfd-ed60-437c-9839-edd34de3d364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35a75a4e-b549-4be4-ae6b-83f49056c782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2.2 Databricks Cluster Optimization Best Practices\n",
    "\n",
    "Cluster tuning in Databricks involves configuring clusters effectively to optimize job performance, reduce costs, and ensure reliability.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Key Areas of Cluster Tuning\n",
    "\n",
    "### 1. Cluster Sizing\n",
    "\n",
    "* Right-size nodes based on workload (compute-intensive vs. memory-intensive).\n",
    "* Suggested: Driver = 1 node, Executors = 2-10 nodes.\n",
    "* Use autoscaling to handle variable loads:\n",
    "\n",
    "```python\n",
    "spark.conf.set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Cluster Type Selection\n",
    "\n",
    "| Cluster Type   | Use Case                |\n",
    "| -------------- | ----------------------- |\n",
    "| All-purpose    | Development, notebooks  |\n",
    "| Job cluster    | Production jobs         |\n",
    "| SQL warehouse  | BI tools, dashboards    |\n",
    "| Photon-enabled | Fast SQL execution      |\n",
    "| Single-node    | Local jobs, ML training |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Memory and Executor Configuration\n",
    "\n",
    "| Setting                        | Description                        |\n",
    "| ------------------------------ | ---------------------------------- |\n",
    "| `spark.executor.memory`        | Memory allocated to each executor  |\n",
    "| `spark.executor.instances`     | Total number of executors          |\n",
    "| `spark.executor.cores`         | Cores per executor                 |\n",
    "| `spark.sql.shuffle.partitions` | Default = 200; tune based on joins |\n",
    "\n",
    "## Use HDD VMs Instead of SSDs for Long-Running Jobs\n",
    "\n",
    "- **Prefer HDD-backed VMs** (e.g., F8) over SSD variants (e.g., F8s), especially for **long-running streaming jobs**.\n",
    "- Monitor the **cluster event logs** for disk-related events like disk expansion needs.\n",
    "- Upgrade node size if disk expansions are frequent.\n",
    "- Note: **HDD VMs do not support disk cache or Delta cache**.\n",
    "---\n",
    "\n",
    "### 4. Autoscaling\n",
    "\n",
    "* Enable autoscaling clusters to grow/shrink based on demand.\n",
    "* Example: `min_workers = 2`, `max_workers = 20`.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Caching and Persistence\n",
    "\n",
    "* Use `.cache()` or `.persist()` if a DataFrame is reused multiple times **and** fits into memory.\n",
    "* Avoid caching large datasets unnecessarily.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Adaptive Query Execution (AQE)\n",
    "\n",
    "Enable AQE for dynamic query optimization:\n",
    "\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "Benefits:\n",
    "\n",
    "* Optimizes join strategies\n",
    "* Skew handling\n",
    "* Dynamic partition tuning\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Garbage Collection (GC) Tuning\n",
    "\n",
    "* Use G1GC to reduce GC overhead:\n",
    "\n",
    "```bash\n",
    "--conf \"spark.executor.extraJavaOptions=-XX:+UseG1GC\"\n",
    "--conf \"spark.driver.extraJavaOptions=-XX:+UseG1GC\"\n",
    "```\n",
    "\n",
    "* Monitor GC time in Spark UI -> Executors tab\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Monitoring and Metrics\n",
    "\n",
    "* Use Spark UI: Stages -> Tasks, Executors -> GC Time\n",
    "* Tools: Ganglia, Datadog, CloudWatch\n",
    "* Enable event logging for deeper analysis\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Cost Optimization Tips\n",
    "\n",
    "| Strategy                           | Benefit                      |\n",
    "| ---------------------------------- | ---------------------------- |\n",
    "| Use spot instances                 | Save 60-80% on compute costs |\n",
    "| Enable auto-termination            | Avoid idle cluster charges   |\n",
    "| Use serverless SQL warehouses      | Auto-scale for SQL queries   |\n",
    "| Schedule OPTIMIZE/VACUUM off-hours | Reduces peak-time load       |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Summary\n",
    "\n",
    "| Tuning Area      | Action                              |\n",
    "| ---------------- | ----------------------------------- |\n",
    "| Sizing           | Choose right executor & memory      |\n",
    "| Cluster type     | Pick based on workload type         |\n",
    "| Autoscaling      | Minimize idle and cost              |\n",
    "| Memory config    | Tune `memory`, `cores`, `instances` |\n",
    "| Adaptive queries | Enable AQE                          |\n",
    "| GC tuning        | Use G1GC and monitor                |\n",
    "| Cost control     | Use spot + serverless + scheduling  |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_spark_optimisation_methods",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
