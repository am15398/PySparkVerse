{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eeb71b63-c39a-461a-9f1e-c832ff5350cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Automating Workflow Jobs with Schedules and Triggers\n",
    "\n",
    "In **Lakeflow Jobs**, it is possible to configure jobs to automatically trigger in any of the following situations:\n",
    "\n",
    "- **On a time-based schedule**\n",
    "- **On the arrival of files** to a Unity Catalog storage location\n",
    "- **Continuously**\n",
    "\n",
    "You can also trigger job runs **manually** or through **external orchestration tools**.\n",
    "\n",
    "---\n",
    "\n",
    "## Job Schedules and Triggers\n",
    "\n",
    "| **Trigger Type** | **Behavior** |\n",
    "|------------------|--------------|\n",
    "| **Scheduled** | Triggers a job run based on a time-based schedule. |\n",
    "| **File arrival** | Triggers a job run when new files arrive in a monitored Unity Catalog storage location. |\n",
    "| **Continuous** | To keep the job always running, trigger another job run whenever a job run completes or fails.|\n",
    "| **None (manual)** | Runs are triggered manually with the **Run now** button or programmatically using other orchestration tools. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ba8f588-1c63-49bc-9a63-a6b49b48a878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🔄 What is Auto Loader in Databricks?\n",
    "\n",
    "**Auto Loader** is a feature in **Databricks** used for **incrementally and efficiently ingesting new data files** as they arrive in cloud storage (like AWS S3, Azure Data Lake, or GCS) into Delta Lake tables.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Key Highlights\n",
    "\n",
    "| Feature              | Description                                                                 |\n",
    "|----------------------|-----------------------------------------------------------------------------|\n",
    "| **Incremental**       | Automatically detects and loads only **new files** added to a directory.    |\n",
    "| **Scalable**          | Designed to **scale to millions of files**, better than using `read`.       |\n",
    "| **Schema Evolution**  | Can **automatically detect new columns** and update the schema if enabled.  |\n",
    "| **State Management**  | Tracks file ingestion state with **checkpoints** to avoid duplicates.       |\n",
    "| **Optimized**         | Uses **cloud-specific APIs** for faster listing (e.g., Azure Event Grid).   |\n",
    "\n",
    "---\n",
    "\n",
    "# 📦 Supported File Formats\n",
    "* CSV\n",
    "* JSON\n",
    "* Parquet\n",
    "* Avro\n",
    "* ORC\n",
    "* Binary\n",
    "\n",
    "---\n",
    "## 📥 Auto Loader `readStream` Options\n",
    "\n",
    "| **Option**                        | **Description**                                         | **Example**            |\n",
    "| --------------------------------- | ------------------------------------------------------- | ---------------------- |\n",
    "| `cloudFiles.format`               | File format (`csv`, `json`, `parquet`, etc.)            | `\"csv\"`                |\n",
    "| `cloudFiles.schemaLocation`       | Required location to store schema metadata              | `\"/mnt/schema/bronze\"` |\n",
    "| `cloudFiles.inferColumnTypes`     | Auto infer column types from data (CSV/JSON only)       | `\"true\"`               |\n",
    "| `cloudFiles.includeExistingFiles` | Process existing files on first run                     | `\"true\"`               |\n",
    "| `cloudFiles.schemaEvolutionMode`  | Auto schema evolution mode (`addNewColumns`)            | `\"addNewColumns\"`      |\n",
    "| `cloudFiles.allowOverwrites`      | Allow file overwrites (if applicable)                   | `\"true\"`               |\n",
    "| `cloudFiles.useNotifications`     | Use notification-based file discovery (Event Grid / S3) | `\"true\"`               |\n",
    "| `cloudFiles.connectionString`     | For Azure Data Lake Gen2: SAS or credentials            | `\"...?sig=...\"`        |\n",
    "| `cloudFiles.partitionColumns`     | Partition columns for file layout                       | `\"year,month\"`         |\n",
    "| `cloudFiles.maxBytesPerTrigger`   | Max data size read per batch                            | `\"104857600\"` (100MB)  |\n",
    "| `cloudFiles.maxFilesPerTrigger`   | Max number of files read per batch                      | `\"100\"`                |\n",
    "| `cloudFiles.enforceSchema`        | If false, allows missing columns instead of failing     | `\"false\"`              |\n",
    "| `cloudFiles.namingHint`           | Helps improve performance in file discovery             | `\"bronze-data\"`        |\n",
    "| `cloudFiles.validateOptions`      | Enables validation of format-specific options           | `\"true\"`               |\n",
    "\n",
    "---\n",
    "## 📤 writeStream Options (Delta Sink)\n",
    "| **Option**           | **Description**                                          | **Example**                           |\n",
    "| -------------------- | -------------------------------------------------------- | ------------------------------------- |\n",
    "| `checkpointLocation` | Required to track stream progress                        | `\"/mnt/checkpoints/myjob/\"`           |\n",
    "| `path`               | Output path if not passed to `.start()`                  | `\"/mnt/delta/bronze/\"`                |\n",
    "| `mergeSchema`        | Merge new schema with existing Delta schema              | `\"true\"`                              |\n",
    "| `outputMode`         | Output mode: `append`, `complete`, `update`              | `\"append\"`                            |\n",
    "| `trigger`            | Controls how frequently batches are triggered            | `Trigger.ProcessingTime(\"5 minutes\")` |\n",
    "| `maxFilesPerTrigger` | Throttles file processing rate                           | `\"100\"`                               |\n",
    "| `maxBytesPerTrigger` | Limits total bytes per micro-batch                       | `\"104857600\"`                         |\n",
    "| `ignoreChanges`      | Ignore updates for existing rows (for idempotent writes) | `\"true\"`                              |\n",
    "| `ignoreDeletes`      | Ignore delete operations if using upserts                | `\"true\"`                              |\n",
    "| `replaceWhere`       | Overwrite subset of data (if `overwrite` mode used)      | `\"year=2024\"`                         |\n",
    "| `partitionBy`        | Partition columns for Delta sink                         | `[\"year\", \"month\"]`                   |\n",
    "---\n",
    "\n",
    "## 🔄 Types of Auto Loader Triggers in Databricks\n",
    "| Trigger Type     | Description                                   | Code Example                             | Use case |\n",
    "| ---------------- | --------------------------------------------- | ---------------------------------------- |------------------\n",
    "| `once`           |**one time micro batch.** Runs once, processes all available data       | `.trigger(once=True)`                    | Nightly or ad-hoc runs when files arrive at a known time. |\n",
    "| `processingTime` | **fixed interval micro batch.** Runs every interval (e.g., 5 min, 1 hr)       | `.trigger(Trigger.ProcessingTime(\"5m\"))` |Near real-time ingestion with controlled resource usage. |\n",
    "| `availableNow`     | **Default.** Runs as many batches as needed to process all currently available data, then stops. | `.trigger(availableNow=True)`     | \n",
    "\n",
    "---\n",
    "## 🔧 How It Works\n",
    "\n",
    "```python\n",
    "\n",
    "from pyspark.sql.functions import input_file_name, current_timestamp\n",
    "\n",
    "# Step 1: Read from cloud storage using Auto Loader\n",
    "df = (\n",
    "  spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\", \"csv\")  # Change to \"json\", \"parquet\", etc. if needed\n",
    "  .option(\"cloudFiles.schemaLocation\", \"/mnt/schema/sales/\")  # Tracks schema changes\n",
    "  .option(\"header\", \"true\")  # Assuming CSV has headers\n",
    "  .load(\"/mnt/raw/mydata/\")  # Folder with 100+ files\n",
    ")\n",
    "\n",
    "# Step 2: Add metadata columns (file name and load timestamp)\n",
    "df_with_metadata = df.withColumn(\"Metadata_source_file_name\", input_file_name()) \\\n",
    "                     .withColumn(\"Metadata_load_timestamp\", current_timestamp())\n",
    "\n",
    "# Step 3: Write to Delta table\n",
    "df_with_metadata.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/mnt/checkpoints/sales_bronze/\") \\\n",
    "    .start(\"/mnt/delta/sales_bronze/\")  # You can also use a managed table path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d17fd3f4-7d3c-434e-bc06-9405f0fd5341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "## 🔄 Auto Loader vs COPY INTO \n",
    "| Feature / Aspect        | 🔄 **Auto Loader**                                          | 📥 **COPY INTO**                                     |\n",
    "| ----------------------- | ----------------------------------------------------------- | ---------------------------------------------------- |\n",
    "| **Ingestion Mode**      | **Streaming** (incremental, continuous or trigger-based)    | **Batch** (manual or scheduled execution)            |\n",
    "| **Source Monitoring**   | Watches a directory for **new files continuously**          | Does **not track** files; must re-check manually     |\n",
    "| **State Management**    | Uses **checkpointing** to track ingested files              | Tracks files using **audit log table metadata**      |\n",
    "| **Latency**             | Near real-time (trigger every few seconds/minutes)          | Manual or scheduled; **higher latency**              |\n",
    "| **Schema Evolution**    | Supported (with `addNewColumns` mode)                       | ❌ Not natively supported                             |\n",
    "| **File Deduplication**  | Built-in using file IDs and checksums                       | Avoids reloading by checking file metadata hash      |\n",
    "| **Trigger Options**     | `once`, `processingTime`, `availableNow`, `continuous`      | No streaming — must be triggered via SQL or jobs     |\n",
    "| **Format Support**      | CSV, JSON, Parquet, Avro, ORC, Binary                       | Same (via `FILEFORMAT`)                              |\n",
    "| **Ease of Use**         | Ideal for large-scale pipelines with many incoming files    | Simpler for small, ad-hoc, or periodic loads         |\n",
    "| **Cost Efficiency**     | Efficient for **frequent ingestion** of large directories   | Better for **one-time** or low-frequency batch loads |\n",
    "| **Catalog Integration** | Fully compatible with Unity Catalog + file arrival triggers | Compatible with Unity Catalog (manual registration)  |\n",
    "---\n",
    "\n",
    "``` sql\n",
    "\n",
    "COPY INTO my_table\n",
    "FROM '/mnt/raw/'\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header' = 'true')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AutoLoader",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
